# Обучение KNN (Никита Королев)

## 1. Выбор метрики

Начнем с выбора метрики: я решил выбрать  **Жаккардову метрику (micro + macro)**. Задача является многоклассовой — у одного поста может быть несколько хабов, поэтому она достаточно неплохо подходит.

**Вспомогательные метрики:**  
- F1-score (micro и macro)  
- Accuracy  
- Label Coverage Rate  

F1-score тоже неплохо подходит для мультиклассовой классификации, accuracy я взял допом (просто интересно). Label Coverage Rate - кастомная метрика, она показывает отношения числа хабов, предсказанных хотя бы 1 раз, к общему числу хабов. В общем, она нужна, чтобы посмотреть, насколько много мы предсказываем классов.

---

## 2. Предобработка данных

- **Данные:** 5000 строк взято для CV и итоговая модель обучалась на 30к строках (дальше объясню, почему не на всем датасете) 
- **Шаги предобработки:**
  - Очистка текстов, заголовков, ключевых слов и ников (аналогичная чекпоинту по EDA)
  - Преобразование текстов в TF-IDF признаки
  - Масштабирование (StandartScaler) числовых признаков (`reading_time`, `timestamp`)
  - Фильтрация редких хабов (менее 350 публикаций) - 
  это сделано из-за того, что большое количество классов убивало расчет, просто все нафиг падало) Выкручивал практически в максимум это параметр, поэтому итоговая модель из рук вон плохо предсказывает непопулярные хабы. С другой стороны - как будто мы хотим обобщить инфу о том, что же происходит в статье, а популярные хабы - это в основном какие-то общие тематики, что в принципе дает инфу о статье в общем виде.

Еще для удобства создал класс `HabrPreprocessor` для объединения всех шагов предобработки в пайплайн.

---

## 3. Подбор параметров и обучение моделей

- **Модель:** `KNN` в схеме `OneVsRestClassifier`.
- **Эксперименты:**
	Повозиться пришлось конкретно. Начал я с простого: TF-IDF с максимальным количеством признаков - итог на 3к строках мой ноутбук умер и перрезагрузился. Начал копать - чтобы я не делал, программа падала по памяти. Дальше - ограничил до 10к фичи в TF-IDF, вроде стало лучше, но с 10к уже опять не работало. Решил сделать такой подход - начну подбирать параметры через CV на очень маленькой выборке и потом попробую обучить на выборке побольше. Начал я с перебора алгоритмов - `ball_tree`, `kd_tree`, `brute`. `brute` считал неадкватно сразу (ну все-таки полный перебор, как никак), остальное улучшило расчет, но все равно, только на маленькой выборке. Я долго боролся с падениями по памяти, но в итоге помогло чудо - то есть параметр `leaf_size`. В общем этот параметр относится к структуре данных алгоритма, и чем он больше, тем быстрее считает `KNN`. Второй момент - это снижение размерности. Сначала попробвал `PCA` - не получилось, так как он нормально не работает с разряженными матрицами (а TF-IDF как раз таки дает разряженную). Получилось сделать через `TruncatedSVD` со 100 компонентами (пробовал 50, 150, 200, 300 - получалось сильно хуже).

	![Итоги](mem.png)
	
	Итого получились такие метркики:
	1) Маленькая выборка (5к), CV
		```bash
		===== Итоговые метрики модели =====
		Label Coverage Rate: 0.7770
		Jaccard (micro): 0.5426
		Jaccard (macro): 0.1999
		F1 (micro):      0.7035
		F1 (macro):      0.2680
		Accuracy:         0.0082
		---------------------------------------
		Лучший скор на кросс-валидации (GridSearch): 0.5511
		Лучшие параметры: {'knn__estimator__n_neighbors': 9}
		```
		Если посмотреть на Jaccard метрику, то при micro - имеем 0.54, при macro имеем 0.2. Если интерпретировать на человеческий, то популярные хабы мы предсказывем лучше (что, в общем то логично, учитывая, что я урезал количество хабоы), а менне популярные - сильно хуже. По F1 мы тоже неплохо предсказываем. Аккураси на самом деле я взял зря, она адекватной оценки не дает, как раз таки из-за наличия нескольких хабов у одной статьи. Если мы посмотрим на метрику кол-ва использованных хабов, то увидим, что мы используем около 80% всех хабов, что достаточно хорошо.
	2) Большая выборка (30к), n_neighbors=3
		```bash
		===== Итоговые метрики модели =====
		Label Coverage Rate: 0.9116
		Jaccard (micro): 0.6076
		Jaccard (macro): 0.3796
		F1 (micro):      0.7559
		F1 (macro):      0.5042
		Accuracy:         0.0648
		```
		На самом деле я очень доволен - если смотреть на Жаккарда, то мы предсказывем (в первом приближении, конечнно, так как метрика Жаккарда это не прям точное предсказания кол-ва угаданных хабов) около 3 популярных хабов их 5, и около 2 непопулярных хабов из 5. Прям круто, учитывая, что я неделю не мог получить больше 0.05 скор)

	Все, что больше 30к у меня падало в рантайме по памяти, можно еще, наверное, что-то улучшить, но пока остается так.

---

## 5. Выводы

На самом деле, как будто `KNN` вообще не очень адекватно подходит для данной задачи как минимум из-за проклятия размерности. Пришлось урезать через `TruncatedSVD`, ограничивать TF-IDF, урезать сильно количество хабов, чтобы хотя бы как-то запустилось. Допольнительно, редкие хабы вообще не появляются среди соседей, поэтому мы в них и попадаем плохо (судя по метркиам). Взял я эту модель, только потому, что в задании к чекпоинту были относительно простые модели, а все остальные модели заняли мои коллеги. Итого моделька дала средненький результат. Что можно сделать дальше? Ну, во-первых, можно использовать модели типа `XGboost`, `RandomForest` и т.п. Если говорить про `KNN` - попробовать добавить `Word2Vec`/`BERT` для признаков, а также попробовать отбалансировать редкие хабы.

