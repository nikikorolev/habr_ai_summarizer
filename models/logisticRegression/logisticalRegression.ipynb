{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "\n",
    "print(\"–ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ...\")\n",
    "df = pd.read_csv('dataset/habr.csv', encoding='utf-8')\n",
    "print(\"–†–∞–∑–º–µ—Ä –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(df)}\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "    text = re.sub(r'[^–∞-—è–∞-—è—ë—ëa-za-z\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def parse_hubs(hub_str):\n",
    "    if pd.isna(hub_str):\n",
    "        return []\n",
    "    try:\n",
    "        return ast.literal_eval(hub_str)\n",
    "    except:\n",
    "        if isinstance(hub_str, str):\n",
    "            return [hub.strip() for hub in hub_str.split(',')]\n",
    "        return []\n",
    "\n",
    "print(\"–û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç—ã...\")\n",
    "df['hubs'] = df['hubs'].apply(parse_hubs)\n",
    "\n",
    "text_columns = ['title', 'keywords', 'text']\n",
    "for col in text_columns:\n",
    "    df[f'cleaned_{col}'] = pd.Series(tqdm(\n",
    "        (preprocess_text(text) for text in df[col]),\n",
    "        total=len(df),\n",
    "        desc=f\"–û–±—Ä–∞–±–æ—Ç–∫–∞ {col}\"\n",
    "    ))\n",
    "\n",
    "df['full_text'] = (\n",
    "    df['cleaned_title'] + \" \" +\n",
    "    df['cleaned_keywords'].fillna(\"\") + \" \" +\n",
    "    df['cleaned_text']\n",
    ")\n",
    "\n",
    "df = df[df['full_text'].str.len() > 50]\n",
    "hub_counts = df['hubs'].explode().value_counts()\n",
    "valid_hubs = hub_counts[hub_counts >= 10].index\n",
    "df['hubs'] = df['hubs'].apply(lambda hubs: [h for h in hubs if h in valid_hubs])\n",
    "df = df[df['hubs'].map(len) >= 3]\n",
    "\n",
    "print(f\"–ò—Ç–æ–≥–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä: {len(df)}\")\n",
    "\n",
    "df.to_pickle(\"processed_habr.pkl\")\n",
    "print(\"–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ processed_habr.pkl\")\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T05:56:29.951758200Z",
     "start_time": "2025-11-18T20:53:53.489995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import jaccard_score, hamming_loss\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from scipy.sparse import vstack\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_pickle(\"data/processed_habr.pkl\")\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['hubs'])\n",
    "print(f\"–•–∞–±—ã: {len(mlb.classes_)}, –°—Ç–∞—Ç—å–∏: {df.shape[0]}\")\n",
    "\n",
    "min_hub_count = 2\n",
    "valid_hub_indices = y.sum(axis=0) >= min_hub_count\n",
    "y_filtered = y[:, valid_hub_indices]\n",
    "\n",
    "print(f\"–ò—Å—Ö–æ–¥–Ω–æ–µ —á–∏—Å–ª–æ —Ö–∞–±–æ–≤: {y.shape[1]}\")\n",
    "print(f\"–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {y_filtered.shape[1]} —Ö–∞–±–æ–≤\")\n",
    "\n",
    "mlb_filtered = MultiLabelBinarizer()\n",
    "filtered_hubs = []\n",
    "for hubs_list in df['hubs']:\n",
    "    filtered_hub = [hub for hub in hubs_list if hub in mlb.classes_[valid_hub_indices]]\n",
    "    filtered_hubs.append(filtered_hub)\n",
    "\n",
    "y = mlb_filtered.fit_transform(filtered_hubs)\n",
    "\n",
    "class_counts = y.sum(axis=0)\n",
    "print(f\"–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ –∫–ª–∞—Å—Å–µ: {class_counts.min()}\")\n",
    "print(f\"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ –∫–ª–∞—Å—Å–µ: {class_counts.max()}\")\n",
    "\n",
    "non_empty_indices = y.sum(axis=1) > 0\n",
    "df = df[non_empty_indices]\n",
    "y = y[non_empty_indices]\n",
    "\n",
    "print(f\"–°—Ç–∞—Ç–µ–π –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –ø—É—Å—Ç—ã—Ö: {len(df)}\")\n",
    "\n",
    "print(\"–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è...\")\n",
    "start_time = time.time()\n",
    "\n",
    "russian_stopwords = [\n",
    "    '–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ',\n",
    "    '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫', '–µ–≥–æ', '–Ω–æ', '–¥–∞'\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=3000,\n",
    "    ngram_range=(1, 1),\n",
    "    min_df=1,\n",
    "    max_df=0.95,\n",
    "    stop_words=russian_stopwords,\n",
    "    sublinear_tf=True,\n",
    "    lowercase=True,\n",
    "    smooth_idf=True\n",
    ")\n",
    "\n",
    "texts = df['full_text'].tolist()\n",
    "X = vectorizer.fit_transform(texts)  \n",
    "\n",
    "print(f\"\\n–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {time.time() - start_time:.2f} —Å–µ–∫\")\n",
    "print(f\"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {X.shape}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42  \n",
    ")\n",
    "\n",
    "print(f\"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_train.shape[0]}\")\n",
    "print(f\"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_test.shape[0]}\")\n",
    "\n",
    "print(\"–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...\")\n",
    "model = MultiOutputClassifier(\n",
    "    LogisticRegression(\n",
    "        max_iter=1000, \n",
    "        class_weight='balanced', \n",
    "        C=0.5, \n",
    "        solver='liblinear', \n",
    "        random_state=42\n",
    "    ),\n",
    "    n_jobs=-1\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# --- –û–¶–ï–ù–ö–ê ---\n",
    "y_pred = model.predict(X_test)\n",
    "jaccard = jaccard_score(y_test, y_pred, average='samples')\n",
    "hamming = hamming_loss(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:\")\n",
    "print(f\"Jaccard Score: {jaccard:.4f}\")\n",
    "print(f\"Hamming Loss: {hamming:.4f}\")\n",
    "\n",
    "# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö\n",
    "print(f\"\\nüìä –î–µ—Ç–∞–ª–∏:\")\n",
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {y.shape[1]}\")\n",
    "print(f\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ –Ω–µ-–Ω—É–ª–µ–π: {(y_pred.sum(axis=1) > 0).sum()} / {y_pred.shape[0]}\")\n",
    "print(f\"–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ö–∞–±–æ–≤ –Ω–∞ —Å—Ç–∞—Ç—å—é: {y.sum(axis=1).mean():.2f}\")"
   ],
   "id": "c519d734e44064e1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–•–∞–±—ã: 955, –°—Ç–∞—Ç—å–∏: 98064\n",
      "–ò—Å—Ö–æ–¥–Ω–æ–µ —á–∏—Å–ª–æ —Ö–∞–±–æ–≤: 955\n",
      "–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: 953 —Ö–∞–±–æ–≤\n",
      "–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ –∫–ª–∞—Å—Å–µ: 2\n",
      "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ –∫–ª–∞—Å—Å–µ: 13906\n",
      "–°—Ç–∞—Ç–µ–π –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –ø—É—Å—Ç—ã—Ö: 98064\n",
      "–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è...\n",
      "\n",
      "–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ 149.50 —Å–µ–∫\n",
      "–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: (98064, 3000)\n",
      "–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: 78451\n",
      "–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: 19613\n",
      "–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...\n",
      "\n",
      "‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:\n",
      "Jaccard Score: 0.2451\n",
      "Hamming Loss: 0.0114\n",
      "\n",
      "üìä –î–µ—Ç–∞–ª–∏:\n",
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: 953\n",
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ –Ω–µ-–Ω—É–ª–µ–π: 19613 / 19613\n",
      "–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ö–∞–±–æ–≤ –Ω–∞ —Å—Ç–∞—Ç—å—é: 3.90\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T05:56:29.954858800Z",
     "start_time": "2025-11-20T05:48:49.581010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import yake\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"–ó–∞–≥—Ä—É–∂–∞–µ–º processed_habr.pkl...\")\n",
    "if not os.path.exists(\"data/processed_habr.pkl\"):\n",
    "    raise FileNotFoundError(\"–§–∞–π–ª processed_habr.pkl –Ω–µ –Ω–∞–π–¥–µ–Ω!\")\n",
    "\n",
    "df = pd.read_pickle(\"data/processed_habr.pkl\")\n",
    "print(f\"–ò—Å—Ö–æ–¥–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(df)}\")\n",
    "\n",
    "N_SAMPLES = 20000\n",
    "if len(df) > N_SAMPLES:\n",
    "    df_sample = df.head(N_SAMPLES).copy()\n",
    "    print(f\"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—ã–±–æ—Ä–∫—É: {len(df_sample)} –∑–∞–ø–∏—Å–µ–π (–ø–µ—Ä–≤—ã–µ {N_SAMPLES})\")\n",
    "else:\n",
    "    df_sample = df.copy()\n",
    "    print(f\"–î–∞—Ç–∞—Å–µ—Ç –º–µ–Ω—å—à–µ {N_SAMPLES}, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ {len(df_sample)} –∑–∞–ø–∏—Å–µ–π\")\n",
    "\n",
    "\n",
    "kw_extractor = yake.KeywordExtractor(\n",
    "    lan=\"ru\",\n",
    "    n=3,\n",
    "    top=15,\n",
    "    dedupLim=0.7,\n",
    "    features=None\n",
    ")\n",
    "\n",
    "russian_stopwords = {\n",
    "    '–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ',\n",
    "    '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫', '–µ–≥–æ', '–Ω–æ', '–¥–∞', '—É', '–∂–µ',\n",
    "    '–±—ã', '–¥–ª—è', '–ø–æ', '–æ', '–æ—Ç', '–∏–∑', '–∫', '–æ–±', '–ø—Ä–∏', '–Ω–∞–¥', '–ø–æ–¥'\n",
    "}\n",
    "\n",
    "def extractKeys(text):\n",
    "    \"\"\"–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã —Å –ø–æ–º–æ—â—å—é YAKE.\"\"\"\n",
    "    if not isinstance(text, str) or len(text.strip()) < 10:\n",
    "        return []\n",
    "    try:\n",
    "        keywords = kw_extractor.extract_keywords(text)\n",
    "        result = []\n",
    "        for phrase, score in keywords:\n",
    "            words = phrase.lower().split()\n",
    "            if any(word not in russian_stopwords for word in words):\n",
    "                result.append(phrase.strip())\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–∫—Å—Ç–∞: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"–ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ cleaned_text —Å –ø–æ–º–æ—â—å—é YAKE...\")\n",
    "df_sample['text_main'] = pd.Series(tqdm(\n",
    "    (extractKeys(text) for text in df_sample['cleaned_text']),\n",
    "    total=len(df_sample),\n",
    "    desc=\"YAKE: –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑\",\n",
    "    unit=\"—Ç–µ–∫—Å—Ç\"\n",
    "))\n",
    "\n",
    "print(\"–°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ—Å–ª–µ YAKE...\")\n",
    "df_sample.to_pickle(\"data/sample_20k_yake_extracted.pkl\")\n",
    "print(\"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: data/sample_20k_yake_extracted.pkl\")\n",
    "\n",
    "print(\"–û—á–∏—â–∞–µ–º –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã...\")\n",
    "\n",
    "def clean_keywords(keywords):\n",
    "    if not isinstance(keywords, list):\n",
    "        return []\n",
    "    \n",
    "    if not keywords:\n",
    "        return []\n",
    "    \n",
    "    cleaned = []\n",
    "    for phrase in keywords:\n",
    "        if not isinstance(phrase, str):\n",
    "            phrase = str(phrase)\n",
    "        \n",
    "        phrase = re.sub(r'[^–∞-—è–ê-–Ø—ë—ëa-zA-Z0-9\\s]', ' ', phrase)\n",
    "        phrase = re.sub(r'\\s+', ' ', phrase).strip()\n",
    "        if phrase:\n",
    "            cleaned.append(phrase)\n",
    "    return cleaned\n",
    "\n",
    "df_sample['text_main'] = df_sample['text_main'].apply(clean_keywords)\n",
    "\n",
    "print(\"–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞: —É–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è...\")\n",
    "def final_clean(keywords):\n",
    "    if not keywords:\n",
    "        return []\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for k in keywords:\n",
    "        if k not in seen:\n",
    "            seen.add(k)\n",
    "            result.append(k)\n",
    "    return result\n",
    "\n",
    "df_sample['text_main'] = df_sample['text_main'].apply(final_clean)\n",
    "\n",
    "print(\"–°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Ç–æ–≥–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ...\")\n",
    "df_sample.to_pickle(\"data/sample_20k_with_keywords.pkl\")\n",
    "df_sample.to_csv(\"data/sample_20k_keywords.csv\", index=False, encoding='utf-8')\n",
    "\n",
    "\n",
    "print(\"\\n–ì–æ—Ç–æ–≤–æ!\")\n",
    "print(f\"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(df_sample)} —Ç–µ–∫—Å—Ç–æ–≤\")\n",
    "print(f\"–ò—Ç–æ–≥–æ–≤—ã–π —Ñ–∞–π–ª: sample_20k_with_keywords.pkl\")\n",
    "print(f\"CSV-–≤–µ—Ä—Å–∏—è: sample_20k_keywords.csv\")\n",
    "\n",
    "print(f\"\\n–ü—Ä–∏–º–µ—Ä –ø–µ—Ä–≤—ã—Ö 15 –∑–Ω–∞—á–µ–Ω–∏–π 'text_main':\")\n",
    "for i in range(15):\n",
    "    if i < len(df_sample):\n",
    "        print(f\"{i}: {df_sample['text_main'].iloc[i]}\")\n"
   ],
   "id": "8e9f6fa7cbac54b0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥—Ä—É–∂–∞–µ–º processed_habr.pkl...\n",
      "–ò—Å—Ö–æ–¥–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: 98064\n",
      "–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—ã–±–æ—Ä–∫—É: 20000 –∑–∞–ø–∏—Å–µ–π (–ø–µ—Ä–≤—ã–µ 20000)\n",
      "–ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ cleaned_text —Å –ø–æ–º–æ—â—å—é YAKE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YAKE: –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20000/20000 [56:15<00:00,  5.92—Ç–µ–∫—Å—Ç/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ—Å–ª–µ YAKE...\n",
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: sample_20k_yake_extracted.pkl\n",
      "–û—á–∏—â–∞–µ–º –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã...\n",
      "–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞: —É–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è...\n",
      "–°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Ç–æ–≥–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ...\n",
      "\n",
      "–ì–æ—Ç–æ–≤–æ!\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: 20000 —Ç–µ–∫—Å—Ç–æ–≤\n",
      "–ò—Ç–æ–≥–æ–≤—ã–π —Ñ–∞–π–ª: sample_20k_with_keywords.pkl\n",
      "CSV-–≤–µ—Ä—Å–∏—è: sample_20k_keywords.csv\n",
      "\n",
      "–ü—Ä–∏–º–µ—Ä –ø–µ—Ä–≤—ã—Ö 15 –∑–Ω–∞—á–µ–Ω–∏–π 'text_main':\n",
      "0: ['–∫–æ–º–ø–∞–Ω–∏—é –ª–∏–±–æ —Å–∞–º—ã–π', '–ª–∏–±–æ —Å–∞–º—ã–π —É–º–Ω—ã–π', '—Å–∞–º—ã–π —É–º–Ω—ã–π –ª–∏–±–æ', '—É–º–Ω—ã–π –ª–∏–±–æ —Å–∞–º—ã–π', '–ª–∏–±–æ —Å–∞–º—ã–π –≥–ª—É–ø—ã–π', '–≤–∏–∫–∏–ø–µ–¥–∏—è –Ω–µ–±–æ–ª—å—à–∞—è –∫–æ–º–ø–∞–Ω–∏—è', '–≤–∏–∫–∏–ø–µ–¥–∏—è –Ω–µ–±–æ–≥–∞—Ç–∞—è –∫–æ–º–ø–∞–Ω–∏—è', '—Å—Ç–∞—Ç–µ–π –≤–∫–ª—é—á–∞—è —Ä–∞–∑–¥–µ–ª—ã', '–Ω–∞–ø—Ä–∏–º–µ—Ä —à–≤–µ–¥—Å–∫–∞—è –≤–∏–∫–∏–ø–µ–¥–∏—è', '—è–∑—ã–∫–æ–≤ –±–ª–∞ –±–ª–∞', '–∏—Å—á–µ–∑–∞—é—â–µ –º–∞–ª—ã –Ω–∞–ø—Ä–∏–º–µ—Ä', '–º–∞–ª—ã –Ω–∞–ø—Ä–∏–º–µ—Ä —Ä–æ–¥–Ω–∞—è', '—Ü–µ–ª—å—é –Ω–∞–±—Ä–∞—Ç—å —Å—Ç–∞—Ç–µ–π', '–ø–æ–¥–∞–≤–ª—è—é—â–∞—è —á–∞—Å—Ç—å —Å—Ç–∞—Ç–µ–π', '–º–∏–ª–ª–∏–∞—Ä–¥–∞—Ö –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ —Å—Ç–∞—Ç–µ–π']\n",
      "1: ['apple newton messagepad', '–¥–æ–ø–æ–ª–Ω–µ–Ω–Ω–æ–π —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ google', '–ø—Ä–æ–¥—É–∫—Ç –∫–æ–º–ø–∞–Ω–∏–∏ google', '–æ—á–∫–∏ google glass', '—Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ google glassgoogle', '—Å–º–∞—Ä—Ç –æ—á–∫–∏ google', '—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞ apple newton', 'glass –ø—Ä–æ–¥—É–∫—Ç –∫–æ–º–ø–∞–Ω–∏–∏', '–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞ dvd –º–æ–≥–ª–∞', '–∞—Ç–º–æ—Å—Ñ–µ—Ä—É –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏', '–≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –∏–Ω–∂–µ–Ω–µ—Ä', '—Å—Ñ–µ—Ä–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏', '–≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ —Ä–µ—à–∏–ª–∞', '—à–ª–µ–º–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏', '–æ—á–∫–∏ –¥–æ–ø–æ–ª–Ω–µ–Ω–Ω–æ–π —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏']\n",
      "2: ['—á–∏—Ç–∞—Ç—å —Å–ª—É—à–∞—Ç—å –æ—Ç–≤–µ—Ç—ã', '–ø–æ–¥–∫–∞—Å—Ç–∞ —á–∏—Ç–∞—Ç—å —Å–ª—É—à–∞—Ç—å', '–∞–∫—É—Å—Ç–∏–∫–∏ —á–∏—Ç–∞—Ç—å —Å–ª—É—à–∞—Ç—å', '–∑–≤—É–∫–∞ —á–∏—Ç–∞—Ç—å —Å–ª—É—à–∞—Ç—å', '—á–∏—Ç–∞—Ç—å —Å–ª—É—à–∞—Ç—å –æ–±—Å—É–∂–¥–∞–µ–º', '—á–∏—Ç–∞—Ç—å —Å–ª—É—à–∞—Ç—å –ø—Ä–æ—Å—Ç—ã–º–∏', '—á–∏—Ç–∞—Ç—å —Å–ª—É—à–∞—Ç—å –≥–æ–≤–æ—Ä–∏–º', '—á–∏—Ç–∞—Ç—å —Å–ª—É—à–∞—Ç—å –∞–∫—É—Å—Ç–∏–∫–∞', '–∫–∞–±–µ–ª–∏ —á–∏—Ç–∞—Ç—å —Å–ª—É—à–∞—Ç—å', '–≤–æ–ø—Ä–æ—Å—ã —á–∏—Ç–∞—Ç—å —Å–ª—É—à–∞—Ç—å', '—á–∏—Ç–∞—Ç—å —Å–ª—É—à–∞—Ç—å', '—Å–ª—É—à–∞—Ç–µ–ª–µ–π —á–∏—Ç–∞—Ç—å —Å–ª—É—à–∞—Ç—å', '—á–∏—Ç–∞—Ç—å —Å–ª—É—à–∞—Ç—å –º–Ω–µ–Ω–∏–µ', '—á–∏—Ç–∞—Ç—å —Å–ª—É—à–∞—Ç—å –º—É–∑—ã–∫–∞–ª—å–Ω—ã–π', '—Å–∏—Ç—É–∞—Ü–∏—è —á–∏—Ç–∞—Ç—å —Å–ª—É—à–∞—Ç—å']\n",
      "3: ['—Ä–µ–≥—É–ª—è—Ä–Ω—ã–π –ø—Ä–∏–µ–º –ø–∞—Ä–∞—Ü–µ—Ç–∞–º–æ–ª–∞', '–∏–Ω—Ç–µ—Ä–≤—å—é –ø—Ä–æ—Ñ–µ—Å—Å–æ—Ä–∞ –∏—Ä–µ–Ω', '–ø–æ–ª–µ–∑–Ω–æ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–π –ø—Ä–∏–µ–º', '–ø—Ä–æ—Ñ–µ—Å—Å–æ—Ä–∞ –∏—Ä–µ–Ω –±—Ä–µ–π—Ç—É—ç–π—Ç', '–∏—Ä–µ–Ω –±—Ä–µ–π—Ç—É—ç–π—Ç –∑–∞–º–µ—Å—Ç–∏—Ç–µ–ª—è', '–∑–µ–ª–∞–Ω–¥–∏—è –∏—Ä–µ–Ω –±—Ä–µ–π—Ç—É—ç–π—Ç', '–∏–Ω—Ñ–µ–∫—Ü–∏–∏ —è–≤–ª—è–µ—Ç—Å—è –≤–∞–∫—Ü–∏–Ω–∞—Ü–∏—è', '–ø—Ä–∏–µ–º –∂–∞—Ä–æ–ø–æ–Ω–∏–∂–∞—é—â–∏—Ö —Å—Ä–µ–¥—Å—Ç–≤', '–≥—Ä–∏–ø–ø–æ–∑–Ω–æ–π –∏–Ω—Ñ–µ–∫—Ü–∏–∏ —è–≤–ª—è–µ—Ç—Å—è', '–±—Ä–µ–π—Ç—É—ç–π—Ç –∑–∞–º–µ—Å—Ç–∏—Ç–µ–ª—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∞', '–Ω–æ–≤–∞—è –∑–µ–ª–∞–Ω–¥–∏—è –∏—Ä–µ–Ω', '–≤–∑—Ä–æ—Å–ª—ã—Ö –ª—é–¥–µ–π –∏–Ω—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö', '—ç—Ñ—Ñ–µ–∫—Ç –¥–µ–π—Å—Ç–≤–∏—è –ø–∞—Ä–∞—Ü–µ—Ç–∞–º–æ–ª–∞', '–¥–µ–π—Å—Ç–≤–∏—è –ø–∞—Ä–∞—Ü–µ—Ç–∞–º–æ–ª–∞ –≥—Ä–∞–º–º', '–ª–µ—á–µ–Ω–∏—é –≤–Ω–µ–±–æ–ª—å–Ω–∏—á–Ω–æ–π –∏–Ω—Ñ–µ–∫—Ü–∏–∏']\n",
      "4: ['—Å–ª—É—á–∞–µ–≤ —Ä–∞–∫–∞ —Å–≤—è–∑–∞–Ω', '–ø—Ä–∏—á–∏–Ω–æ–π —Å–ª—É—á–∞–µ–≤ —Ä–∞–∫–∞', '–∏–∑–±–µ–∂–∞—Ç—å —Å–ª—É—á–∞–µ–≤ —Ä–∞–∫–∞', '–ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å —Å–ª—É—á–∞–µ–≤ —Ä–∞–∫–∞', '—Å–µ–º–∏ —Å–ª—É—á–∞–µ–≤ —Ä–∞–∫–∞', '—Ä–∞–∫–∞ —Ä–∞–∫–∞ –ª—ë–≥–∫–∏—Ö', '—Ä–∏—Å–∫ —Ä–∞–∫–∞ –≥—Ä—É–¥–∏', '—Ä–∞–∫–∞ –∫–æ–ª–æ—Ä–µ–∫—Ç–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∫–∞', '–∫–æ–ª–æ—Ä–µ–∫—Ç–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∫–∞ —Ä–∞–∫–∞', '—Å–ª—É—á–∞–µ–≤ —Ä–∞–∫–∞ –∞–Ω—É—Å–∞', '—Å–ª—É—á–∞–µ–≤ —Ä–∞–∫–∞ –ø–æ–ª–æ–≤–æ–≥–æ', '—Å–ª—É—á–∞–µ–≤ –≤–∞–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∫–∞', '—Å–ª—É—á–∞–µ–≤ –º–µ–ª–∞–Ω–æ–º—ã —Ä–∞–∫–∞', '—à–∞–Ω—Å –≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏—è —Ä–∞–∫–∞', '–∞–ª—å–±–µ—Ä—Ç–µ –≤—ã–∑—ã–≤–∞–µ—Ç —Å–ª—É—á–∞–µ–≤']\n",
      "5: ['–ø—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫–∞–Ω–∞–ª–∞', '–Ω–µ–¥–æ–≤–æ–ª–µ–Ω –ø—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å', '—Å—Ä–µ–¥–Ω—è—è –ø—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å', '–ø—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Å–µ—Ç–µ–≤–æ–≥–æ', '—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Å–µ—Ç–µ–≤–æ–≥–æ –ø–æ—Ä—Ç–∞', 'zero —Ñ–æ—Ç–æ raspberry', '–ø–æ—Ä—Ç–∞ –º–∞–ª–∏–Ω–∫–∏ –º–±–∏—Ç', '—Å–ª–æ–≤–∞–º —Å—Ä–µ–¥–Ω—è—è –ø—Ä–æ–ø—É—Å–∫–Ω–∞—è', '—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫–∞–Ω–∞–ª–∞ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç–æ–∫–æ–ª–æ', '—Å–æ–æ–±—â–µ–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–µ hey', '–Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Ä–µ–≥–∏–æ–Ω–∞—Ö —Å–∏—Ç—É–∞—Ü–∏—è', '—Å—Ç–æ–ª–∏—Ü–µ —Å—à–∞ –≤–∞—à–∏–Ω–≥—Ç–æ–Ω–µ', '–∂–∏—Ç–µ–ª—å –≤–∞—à–∏–Ω–≥—Ç–æ–Ω–∞ –∏–∑–≤–µ—Å—Ç–µ–Ω', '—Ä–µ—à–∏–ª –Ω–∞–ª–∞–¥–∏—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é', '–Ω–∞–ª–∞–¥–∏—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –æ—Ç–ø—Ä–∞–≤–∫—É']\n",
      "6: ['–ø–∞—Ä–∞–±–æ–ª–∏—á–µ—Å–∫–∏—Ö –∑–µ—Ä–∫–∞–ª —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω—ã', '—Å—Ç–æ–∏–º–æ—Å—Ç—å –º–ª–Ω –µ–≤—Ä–æ', 'noor iii –º–≤—Ç', '–º–µ—Ç—Ä–æ–≤—ã—Ö –ø–∞—Ä–∞–±–æ–ª–∏—á–µ—Å–∫–∏—Ö –∑–µ—Ä–∫–∞–ª', 'iii –º–≤—Ç –º–ª–Ω', '–º–≤—Ç –º–ª–Ω –µ–≤—Ä–æ', '—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –ø—Ä–æ–µ–∫—Ç–∞ noor', '–±–æ–ª—å—à–∏—Ö –º–µ—Ç—Ä–æ–≤—ã—Ö –ø–∞—Ä–∞–±–æ–ª–∏—á–µ—Å–∫–∏—Ö', 'csp —Å—Ç–∞–Ω—Ü–∏–∏ noor', '—ç–Ω–µ—Ä–≥–∏–∏ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ noor', '–º–≤—Ç —Å—Ç–æ–∏–º–æ—Å—Ç—å –º–ª–Ω', '–ø—É—Å—Ç—ã–Ω–µ —Å–∞—Ö–∞—Ä–∞ —Å–æ—Å—Ç–æ—è–ª–∞—Å—å—Ü–µ—Ä–µ–º–æ–Ω–∏—è', '—Å–∞—Ö–∞—Ä–∞ —Å–æ—Å—Ç–æ—è–ª–∞—Å—å—Ü–µ—Ä–µ–º–æ–Ω–∏—è —Ç–æ—Ä–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ', '—Å–æ—Å—Ç–æ—è–ª–∞—Å—å—Ü–µ—Ä–µ–º–æ–Ω–∏—è —Ç–æ—Ä–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –æ—Ç–∫—Ä—ã—Ç–∏—è–∫—Ä—É–ø–Ω–µ–π—à–µ–π', 'concentrating solar power']\n",
      "7: ['–≥–µ–Ω–æ–º—ã –∫–ª–æ–ø–æ–≤ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö', '—Ä–µ—à–µ–Ω–∏–µ —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞ –≥–µ–Ω–æ–º–∞', '—Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞ –≥–µ–Ω–æ–º–∞ –∫–ª–æ–ø–∞', '—Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞ –≥–µ–Ω–æ–º–∞ —Ç–∞–∫–æ–≥–æ', '–ø—Ä–∏—à–ª–æ—Å—å —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å –≥–µ–Ω–æ–º—ã', '—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —É–¥–∏–≤–ª—è—é—Ç –≥–µ–Ω–æ–º—ã', '–Ω–∞–ø—Ä–∏–º–µ—Ä –æ—Ç–ª–∏—á–∞—é—Ç—Å—è –≥–µ–Ω–æ–º—ã', '—Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å –≥–µ–Ω–æ–º—ã –∫–ª–æ–ø–æ–≤', '–≥–µ–Ω–æ–º—ã –∫–ª–æ–ø–æ–≤ –Ω–∞—á–∏–Ω–∞—è', '—É–¥–∏–≤–ª—è—é—Ç –≥–µ–Ω–æ–º—ã –∫–ª–æ–ø–æ–≤', '–≥–µ–Ω–æ–º—ã –∫–ª–æ–ø–æ–≤ —Ä–∞–∑–ª–∏—á–∞—é—Ç—Å—è', '–æ—Ç–ª–∏—á–∞—é—Ç—Å—è –≥–µ–Ω–æ–º—ã –∫–ª–æ–ø–æ–≤', '—Ñ–æ—Ç–æ brian kersey', '–æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –±–æ—Ä—å–±—ã', '–≥–µ–Ω–æ–º–∞ —Ç–∞–∫–æ–≥–æ –æ—Ä–≥–∞–Ω–∏–∑–º–∞']\n",
      "8: ['usb –¥–∏—Å–∫–æ–≤–æ–¥–µ usb', '—É–º–µ–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å usb', 'dos –∏–≥—Ä –Ω–∞–∫–æ–ø–∏—Ç–µ–ª—å', 'usb –∫–ª–∞–≤–∏–∞—Ç—É—Ä–∞ –∏—Ç–∞–∫', '–¥–∏—Å–∫–æ–≤–æ–¥–µ usb –∫–ª–∞–≤–∏–∞—Ç—É—Ä–∞', '—Ä–∞–±–æ—Ç–∞—Ç—å usb –∫–ª–∞–≤–∏–∞—Ç—É—Ä–∞', 'usb —Ñ–ª–æ–ø–ø–∏ –¥–∏—Å–∫–æ–≤–æ–¥–µ', '—Å—Ç–∞—Ä—ã—Ö dos –∏–≥—Ä', '–∏–∑–≤–µ—Å—Ç–Ω—ã–µ –∏–≥—Ä—ã –Ω–∞–ø—Ä–∏–º–µ—Ä', 'dos —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∑–∏–ª—Å—è', '—Ü–µ–ª—å—é –Ω–∞—à–µ–≥–æ –ø—É—Ç–µ—à–µ—Å—Ç–≤–∏—è', '—Ä–∞—Ä–∏—Ç–µ—Ç–Ω—ã–π –∫–æ–º–ø—å—é—Ç–µ—Ä –ø–æ—ç—Ç–æ–º—É', '—Ñ–ª–µ—à–∫–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–æ—Å—Ç–∞–≤–∏—Ç—å', '–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–ª–∞–±—ã–π –∫–æ–º–ø—å—é—Ç–µ—Ä', '—Å–∞–º—ã–µ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –∏–≥—Ä—ã']\n",
      "9: ['–æ–±–ª–∞–∫–µ –≥–∞–∑–∞ –ø—É—Å–∫–∞–π', '–æ–±–ª–∞–∫–µ –¥–æ–ª–∂–Ω—ã –Ω–∞—Ö–æ–¥–∏—Ç—å—Å—è', '–æ—Ç–∫—É–¥–∞ –≤—ã–±—Ä–æ—à–µ–Ω–æ –æ–±–ª–∞–∫–æ', '–≥–∞–ª–∞–∫—Ç–∏–∫–∏ –æ—Ç–∫—É–¥–∞ –≤—ã–±—Ä–æ—à–µ–Ω–æ', '–≤–µ—Ä–Ω–µ—Ç—Å—è –æ–±–ª–∞–∫–æ —Å–º–∏—Ç', '–æ–±–ª–∞–∫–æ —è–≤–ª—è–µ—Ç—Å—è –ø—Ä–∏–º–µ—Ä–æ–º', '–¥–∏—Å–∫–µ –≥–∞–ª–∞–∫—Ç–∏–∫–∏ –æ—Ç–∫—É–¥–∞', '–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å—Å—è –ø—Ä–∏–º–µ—Ä–Ω–æ –º–ª–Ω', '–ø—Ä–∏–º–µ—Ä–Ω–æ –º–ª–Ω –Ω–æ–≤—ã—Ö', '–Ω–∞–∑–≤–∞–Ω–Ω–æ–µ –æ–±–ª–∞–∫–æ–º —Å–º–∏—Ç', '–ø–µ—Ä–≤–æ–æ—Ç–∫—Ä—ã–≤–∞—Ç–µ–ª—å–Ω–∏—Ü—ã –≥–µ–π–ª—ã —Å–º–∏—Ç', '–≥–µ–π–ª—ã —Å–º–∏—Ç –æ–±–Ω–∞—Ä—É–∂–∏–≤—à–µ–π', '—Å–º–∏—Ç –æ–±–Ω–∞—Ä—É–∂–∏–≤—à–µ–π –æ–±—ä–µ–∫—Ç', '—Å–º–∏—Ç –∑–∞—Å–µ–∫–ª–∞ —Ä–∞–¥–∏–æ–≤–æ–ª–Ω—ã', '–≥–ª–∞–∑–∞ —á–µ–ª–æ–≤–µ–∫–∞ –æ–±–ª–∞–∫–æ']\n",
      "10: ['—Ä–∞–Ω–Ω–µ–π —Å—Ç–∞–¥–∏–∏ —Å–∏–≥–Ω–∞–ª—å–Ω–∞—è', '–ø—è—Ç–∏ —Ä–∞–∑–Ω—ã—Ö –≤–∏–¥–æ–≤', '–ø—è—Ç–∏ —Ä–∞–∑–Ω—ã—Ö –≤–∏–¥–∞—Ö', '—Ä–∞–∑–Ω—ã—Ö –≤–∏–¥–æ–≤ —Ä–∞–∫–∞', '—Å—Ç–∞–¥–∏–∏ —Å–∏–≥–Ω–∞–ª—å–Ω–∞—è –≥–µ–Ω–æ–º–Ω–∞—è', '—Ä–∞–∑–Ω—ã—Ö –≤–∏–¥–∞—Ö —Ä–∞–∫–∞', '–≤—ã—è–≤–∏–ª–∏ –≥–µ–Ω–æ–º–Ω—É—é —Å–∏–≥–Ω–∞—Ç—É—Ä—É', '–Ω–∞—à–ª–∏ –º–µ—Ç–∏–ª–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏–≥–Ω–∞—Ç—É—Ä—É', '–≤—ã—è–≤–∏–ª–∏ —É–Ω–∏–∫–∞–ª—å–Ω—É—é —Å–∏–≥–Ω–∞—Ç—É—Ä—É', '–±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –≤–∏–¥–æ–≤', '–∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –≤–∏–¥–æ–≤ —Ä–∞–∫–∞', '–≤–∏–¥–æ–≤ —Ä–∞–∫–∞ –≤–æ—Å—Ö–∏—â–µ–Ω–∏–µ', '–∫—Ä–æ–≤–∏ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å —Å—Ä–∞–∑—É', '–∞–Ω–∞–ª–∏–∑–æ–º –∫—Ä–æ–≤–∏ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å', '–±–∏–æ–ª–æ–≥ –ª–∞—É—Ä–∞ –µ–ª—å–Ω–∏—Ç—Å–∫–∏']\n",
      "11: ['—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º –¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç–æ–º alphabet', '–º–ª—Ä–¥ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –º–ª—Ä–¥', '–¥–æ—Ö–æ–¥–∞ alphabet –º–ª—Ä–¥', 'alphabet –º–ª—Ä–¥ –æ–±–µ—Å–ø–µ—á–∏–ª–∏', '–∫–æ–º–ø–∞–Ω–∏—è google alphabet', 'alphabet —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–æ–º–ø–∞–Ω–∏–∏', 'google alphabet –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏–ª–∞', '–∫–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–æ–º–ø–∞–Ω–∏–π apple', '–¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç–æ–º alphabet —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã', '—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã google', '–ø–æ–¥—Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º alphabet google', '—Å–µ—Ä–≤–∏—Å–∞ google alphabet', '–º–ª—Ä–¥ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ –≤—ã—Ä—É—á–∫–∞', 'alphabet –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏–ª–∞ –∫–≤–∞—Ä—Ç–∞–ª—å–Ω—ã–π', 'google –æ–±–≥–æ–Ω—è–ª–∞ apple']\n",
      "12: ['–∫–∞–º–µ—Ä–æ–π mp–∑–∞ –±–µ—Å–ø–ª–∞—Ç–Ω–æ', 'geektimes –ø—Ä–∏—è—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞', 'mp–∑–∞ –±–µ—Å–ø–ª–∞—Ç–Ω–æ –¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è', '—Ä–æ—Å—Å–∏—é —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∞–∫–∫—É–º—É–ª—è—Ç–æ—Ä', 'cm–≤–µ—Å –≥—Ä–º–∞—Ç–µ—Ä–∏–∞–ª –ø–ª–∞—Å—Ç–∏–∫–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ', '–≥—Ä–º–∞—Ç–µ—Ä–∏–∞–ª –ø–ª–∞—Å—Ç–∏–∫–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–∞–ª–æ–≤', '–ø–ª–∞—Å—Ç–∏–∫–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–∞–ª–æ–≤ —á–∞—Å—Ç–æ—Ç–∞', 'ghz–¥–∞–ª—å–Ω–æ—Å—Ç—å —Å–≤—è–∑–∏ –º–µ—Ç—Ä–æ–≤–∫–∞–º–µ—Ä–∞', '–∫–æ–º–ø–ª–µ–∫—Ç–∞—Ü–∏–∏ –∫–æ–º–ø–ª–µ–∫—Ç–∞—Ü–∏—è –ø–æ—Å—Ç–∞–≤–∫–∏', 'usb –∑–∞—Ä—è–¥–∫–∞ –∫–∞–±–µ–ª—å', '–ª—é–±—ã–µ –ø—Ä–µ–ø—è—Ç—Å—Ç–≤–∏—è —Å–æ–µ–¥–∏–Ω—è—è', '–≤—ã–±–æ—Ä–∞ —Å–ª–æ–∂–Ω–æ–π —Ç–µ—Ö–Ω–∏–∫–∏dronk', '—Å—Ç–æ–∏–º–æ—Å—Ç–∏ –ø–æ–∫—É–ø–∫–∏ –ø—Ä–∏–æ–±—Ä–µ—Ç–∞—è', '–ø–æ–∫—É–ø–∫–∏ –ø—Ä–∏–æ–±—Ä–µ—Ç–∞—è —Ç–æ–≤–∞—Ä', '—Å—Å—ã–ª–∫–∞–º –ø–æ–¥—Ä–æ–±–Ω–µ–µ dronk']\n",
      "13: ['–∫–∞—á–µ—Å—Ç–≤–µ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞', '–∫–∞—á–µ—Å—Ç–≤–µ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è', '—Ñ—É–Ω–∫—Ü–∏—è –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏', '—Ç–∞–∫–æ–π—à–ª–µ–º –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏', '—Å–æ–≤—Å–µ–ºgoogle cardboard –Ω–æ–≤–æ–µ', '–≤–µ—Ä—Å–∏—è google cardboard', '–∫–∞—Å–∞–µ—Ç—Å—è google cardboard', '–≤–∏—Ä—Ç—É–∞–ª—å–Ω—É—é —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—å google', 'google cardboard —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ', '–∫–æ—Ä–ø–æ—Ä–∞—Ü–∏—è –ø–ª–∞–Ω–∏—Ä—É–µ—Ç –≤—ã–ø—É—Å—Ç–∏—Ç—å', '–ø–ª–∞–Ω–∏—Ä—É–µ—Ç –≤—ã–ø—É—Å—Ç–∏—Ç—å –¥–µ–≤–∞–π—Å', '–≤—ã–ø—É—Å—Ç–∏—Ç—å –æ—á–µ—Ä–µ–¥–Ω—É—é –º–æ–¥–µ–ª—å', '–ª–∏–Ω–∑—ã –ø–ª—é—Å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è', '–æ—á–µ—Ä–µ–¥–Ω—É—é –º–æ–¥–µ–ª—å –≤–∏–¥–µ–æ–æ—á–∫–æ–≤', '–ø–∏—à–µ—Çbusinessinsider –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏—è –ø–ª–∞–Ω–∏—Ä—É–µ—Ç']\n",
      "14: ['—Å–∞–º–æ–ª—ë—Ç–∞–º –∏–ª–æ–Ω –º–∞—Å–∫', '–∏—Å—Ç–æ—á–Ω–∏–∫ –ø–∏—Ç–∞–Ω–∏—è –æ–∫–∞–∑–∞–ª—Å—è', '–≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –ø–∏—Ç–∞–Ω–∏—è', '–ø–æ–ª–Ω–æ—Å—Ç—å—é —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∏–π —Å–∞–º–æ–ª—ë—Ç', '–º–∞—Å–∫ —Ö–æ—á–µ—Ç —Å–æ–∑–¥–∞—Ç—å', '–ø–∏—Ç–∞–Ω–∏—è –æ–∫–∞–∑–∞–ª—Å—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ', '–ª–∏—Ç–∏–π –∏–æ–Ω–Ω—ã–µ –∞–∫–∫—É–º—É–ª—è—Ç–æ—Ä—ã', '—Ö–æ—á–µ—Ç –¥–µ–ª–∞—Ç—å —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∏–µ', '—É–ø–æ–º–∏–Ω–∞–µ—Ç —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∏–µ —Å–∞–º–æ–ª—ë—Ç—ã', '—ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∏–µ —Å–∞–º–æ–ª—ë—Ç—ã –≤—Å–ø–æ–º–∏–Ω–∞—é—Ç–∂—É—Ä–Ω–∞–ª–∏—Å—Ç—ã', '—Å–∞–º–æ–ª—ë—Ç–æ–≤ –ø—ã—Ç–∞–ª–∏—Å—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å', '–¥–µ–ª–∞—Ç—å —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∏–µ –¥–≤–∏–≥–∞—Ç–µ–ª–∏', '–ª–µ—Ç–æ–º —Ä–µ–∞–ª—å–Ω–æ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å', '–ø—Ä–æ–µ–∫—Ç —Ä–µ–∞–ª—å–Ω–æ –Ω–∞—á–Ω—É—Ç', '—Ä–µ–∞–ª—å–Ω–æ –Ω–∞—á–Ω—É—Ç –≤–æ–æ–±—â–µ']\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T05:56:29.954858800Z",
     "start_time": "2025-11-20T08:17:46.959401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import jaccard_score, hamming_loss, f1_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from scipy.sparse import hstack  \n",
    "import time\n",
    "\n",
    "df = pd.read_pickle(\"data/sample_20k_with_keywords.pkl\")\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['hubs'])\n",
    "valid_hub_mask = y.sum(axis=0) >= 2 \n",
    "y_filtered = y[:, valid_hub_mask]\n",
    "\n",
    "filtered_hubs = [\n",
    "    [hub for hub in hubs if hub in mlb.classes_[valid_hub_mask]]\n",
    "    for hubs in df['hubs']\n",
    "]\n",
    "mlb_filtered = MultiLabelBinarizer()\n",
    "y = mlb_filtered.fit_transform(filtered_hubs)\n",
    "\n",
    "non_empty = y.sum(axis=1) > 0\n",
    "df = df[non_empty].copy()\n",
    "y = y[non_empty]\n",
    "\n",
    "print(f\"–°—Ç–∞—Ç–µ–π –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(df)}, —Ö–∞–±–æ–≤: {y.shape[1]}\")\n",
    "\n",
    "print(\"–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è...\")\n",
    "start_time = time.time()\n",
    "\n",
    "df['text_main_str'] = df['text_main'].apply(lambda x: ' '.join(x) if x else '')\n",
    "df['cleaned_keywords_str'] = df['cleaned_keywords'].fillna('').astype(str)\n",
    "df['full_text_with_yake'] = (\n",
    "    df['cleaned_title'].fillna('') + ' ' +\n",
    "    df['cleaned_keywords_str'] + ' ' +\n",
    "    df['text_main_str']\n",
    ")\n",
    "\n",
    "mask = df['full_text_with_yake'].str.len() > 50\n",
    "df = df[mask]\n",
    "y = y[mask.values] \n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=20000,\n",
    "    ngram_range=(1, 4),\n",
    "    min_df=3,\n",
    "    max_df=0.75,\n",
    "    stop_words=[\n",
    "        '–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ',\n",
    "        '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫', '–µ–≥–æ', '–Ω–æ', '–¥–∞'\n",
    "    ],\n",
    "    sublinear_tf=True,\n",
    "    lowercase=True,\n",
    "    smooth_idf=True,\n",
    "    norm='l2'\n",
    ")\n",
    "X_text = vectorizer.fit_transform(df['full_text_with_yake'])\n",
    "\n",
    "username_vectorizer = TfidfVectorizer(\n",
    "    analyzer='char_wb',      \n",
    "    ngram_range=(2, 4),    \n",
    "    min_df=1,             \n",
    "    max_features=500,     \n",
    "    sublinear_tf=True,\n",
    "    lowercase=True\n",
    ")\n",
    "X_username = username_vectorizer.fit_transform(df['username'].fillna(''))\n",
    "\n",
    "X = hstack([X_text, X_username])  \n",
    "\n",
    "print(f\"!–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∑–∞ {time.time() - start_time:.2f} —Å–µ–∫, X: {X.shape}, y: {y.shape}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...\")\n",
    "modelNew = MultiOutputClassifier(\n",
    "    LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        class_weight='balanced',\n",
    "        C=0.5,\n",
    "        solver='liblinear',\n",
    "        random_state=42\n",
    "    ),\n",
    "    n_jobs=-1\n",
    ")\n",
    "modelNew.fit(X_train, y_train)\n",
    "\n",
    "y_pred = modelNew.predict(X_test)\n",
    "jaccard = jaccard_score(y_test, y_pred, average='samples')\n",
    "hamming = hamming_loss(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:\")\n",
    "print(f\"Jaccard Score: {jaccard:.4f}\")\n",
    "print(f\"Hamming Loss:  {hamming:.4f}\")\n",
    "print(f\"–ö–ª–∞—Å—Å–æ–≤: {y.shape[1]}, —Å—Ç–∞—Ç–µ–π: {len(df)}\")\n"
   ],
   "id": "77fe4065e637999f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°—Ç–∞—Ç–µ–π –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: 20000, —Ö–∞–±–æ–≤: 698\n",
      "–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è...\n",
      "!–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∑–∞ 9.94 —Å–µ–∫, X: (19764, 20500), y: (19764, 698)\n",
      "–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...\n",
      "\n",
      "‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:\n",
      "Jaccard Score: 0.3788\n",
      "Hamming Loss:  0.0073\n",
      "–ö–ª–∞—Å—Å–æ–≤: 698, —Å—Ç–∞—Ç–µ–π: 19764\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T05:56:29.955850Z",
     "start_time": "2025-11-20T14:44:54.386654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import yake\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"–ó–∞–≥—Ä—É–∂–∞–µ–º processed_habr.pkl...\")\n",
    "if not os.path.exists(\"data/processed_habr.pkl\"):\n",
    "    raise FileNotFoundError(\"–§–∞–π–ª processed_habr.pkl –Ω–µ –Ω–∞–π–¥–µ–Ω!\")\n",
    "\n",
    "df = pd.read_pickle(\"data/processed_habr.pkl\")\n",
    "print(f\"–ò—Å—Ö–æ–¥–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(df)}\")\n",
    "\n",
    "df_sample = df.copy()\n",
    "\n",
    "kw_extractor = yake.KeywordExtractor(\n",
    "    lan=\"ru\",\n",
    "    n=3,\n",
    "    top=15,\n",
    "    dedupLim=0.7,\n",
    "    features=None\n",
    ")\n",
    "\n",
    "russian_stopwords = {\n",
    "    '–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ',\n",
    "    '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫', '–µ–≥–æ', '–Ω–æ', '–¥–∞', '—É', '–∂–µ',\n",
    "    '–±—ã', '–¥–ª—è', '–ø–æ', '–æ', '–æ—Ç', '–∏–∑', '–∫', '–æ–±', '–ø—Ä–∏', '–Ω–∞–¥', '–ø–æ–¥'\n",
    "}\n",
    "\n",
    "def extractKeys(text):\n",
    "    if not isinstance(text, str) or len(text.strip()) < 10:\n",
    "        return []\n",
    "    try:\n",
    "        keywords = kw_extractor.extract_keywords(text)\n",
    "        result = []\n",
    "        for phrase, score in keywords:\n",
    "            words = phrase.lower().split()\n",
    "            if any(word not in russian_stopwords for word in words):\n",
    "                result.append(phrase.strip())\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–∫—Å—Ç–∞: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"–ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ cleaned_text —Å –ø–æ–º–æ—â—å—é YAKE...\")\n",
    "df_sample['text_main'] = pd.Series(tqdm(\n",
    "    (extractKeys(text) for text in df_sample['cleaned_text']),\n",
    "    total=len(df_sample),\n",
    "    desc=\"YAKE: –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑\",\n",
    "    unit=\"—Ç–µ–∫—Å—Ç\"\n",
    "))\n",
    "\n",
    "print(\"–°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ—Å–ª–µ YAKE...\")\n",
    "df_sample.to_pickle(\"data_with_main_info_extract.pkl\")\n",
    "print(\"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: data_with_main_info_extract.pkl\")\n",
    "\n",
    "print(\"–û—á–∏—â–∞–µ–º –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã...\")\n",
    "\n",
    "def clean_keywords(keywords):\n",
    "    if not isinstance(keywords, list):\n",
    "        return []\n",
    "    \n",
    "    if not keywords:\n",
    "        return []\n",
    "    \n",
    "    cleaned = []\n",
    "    for phrase in keywords:\n",
    "        if not isinstance(phrase, str):\n",
    "            phrase = str(phrase)\n",
    "        \n",
    "        phrase = re.sub(r'[^–∞-—è–ê-–Ø—ë—ëa-zA-Z0-9\\s]', ' ', phrase)\n",
    "        phrase = re.sub(r'\\s+', ' ', phrase).strip()\n",
    "        if phrase:\n",
    "            cleaned.append(phrase)\n",
    "    return cleaned\n",
    "\n",
    "df_sample['text_main'] = df_sample['text_main'].apply(clean_keywords)\n",
    "\n",
    "print(\"–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞: —É–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è...\")\n",
    "def final_clean(keywords):\n",
    "    if not keywords:\n",
    "        return []\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for k in keywords:\n",
    "        if k not in seen:\n",
    "            seen.add(k)\n",
    "            result.append(k)\n",
    "    return result\n",
    "\n",
    "df_sample['text_main'] = df_sample['text_main'].apply(final_clean)\n",
    "\n",
    "print(\"–°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Ç–æ–≥–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ...\")\n",
    "df_sample.to_pickle(\"data_with_main_info.pkl\")\n",
    "df_sample.to_csv(\"data_with_main_info.csv\", index=False, encoding='utf-8')\n",
    "\n",
    "\n",
    "print(\"\\n–ì–æ—Ç–æ–≤–æ!\")\n",
    "print(f\"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(df_sample)} —Ç–µ–∫—Å—Ç–æ–≤\")\n",
    "print(f\"–ò—Ç–æ–≥–æ–≤—ã–π —Ñ–∞–π–ª: data_with_main_info.pkl\")\n",
    "print(f\"CSV-–≤–µ—Ä—Å–∏—è: data_with_main_info.csv\")\n",
    "\n",
    "print(f\"\\n–ü—Ä–∏–º–µ—Ä –ø–µ—Ä–≤—ã—Ö 15 –∑–Ω–∞—á–µ–Ω–∏–π 'text_main':\")\n",
    "for i in range(15):\n",
    "    if i < len(df_sample):\n",
    "        print(f\"{i}: {df_sample['text_main'].iloc[i]}\")\n"
   ],
   "id": "170df86afa91fa36",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥—Ä—É–∂–∞–µ–º processed_habr.pkl...\n",
      "–ò—Å—Ö–æ–¥–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: 98064\n",
      "–ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ cleaned_text —Å –ø–æ–º–æ—â—å—é YAKE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YAKE: –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 98064/98064 [4:36:28<00:00,  5.91—Ç–µ–∫—Å—Ç/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ—Å–ª–µ YAKE...\n",
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: data_with_main_info_extract.pkl\n",
      "–û—á–∏—â–∞–µ–º –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã...\n",
      "–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞: —É–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è...\n",
      "–°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Ç–æ–≥–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ...\n",
      "\n",
      "–ì–æ—Ç–æ–≤–æ!\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: 98064 —Ç–µ–∫—Å—Ç–æ–≤\n",
      "–ò—Ç–æ–≥–æ–≤—ã–π —Ñ–∞–π–ª: data_with_main_info.pkl\n",
      "CSV-–≤–µ—Ä—Å–∏—è: data_with_main_info.csv\n",
      "\n",
      "–ü—Ä–∏–º–µ—Ä –ø–µ—Ä–≤—ã—Ö 15 –∑–Ω–∞—á–µ–Ω–∏–π 'text_main':\n",
      "0: ['–∫–æ–º–ø–∞–Ω–∏—é –ª–∏–±–æ —Å–∞–º—ã–π', '–ª–∏–±–æ —Å–∞–º—ã–π —É–º–Ω—ã–π', '—Å–∞–º—ã–π —É–º–Ω—ã–π –ª–∏–±–æ', '—É–º–Ω—ã–π –ª–∏–±–æ —Å–∞–º—ã–π', '–ª–∏–±–æ —Å–∞–º—ã–π –≥–ª—É–ø—ã–π', '–≤–∏–∫–∏–ø–µ–¥–∏—è –Ω–µ–±–æ–ª—å—à–∞—è –∫–æ–º–ø–∞–Ω–∏—è', '–≤–∏–∫–∏–ø–µ–¥–∏—è –Ω–µ–±–æ–≥–∞—Ç–∞—è –∫–æ–º–ø–∞–Ω–∏—è', '—Å—Ç–∞—Ç–µ–π –≤–∫–ª—é—á–∞—è —Ä–∞–∑–¥–µ–ª—ã', '–Ω–∞–ø—Ä–∏–º–µ—Ä —à–≤–µ–¥—Å–∫–∞—è –≤–∏–∫–∏–ø–µ–¥–∏—è', '—è–∑—ã–∫–æ–≤ –±–ª–∞ –±–ª–∞', '–∏—Å—á–µ–∑–∞—é—â–µ –º–∞–ª—ã –Ω–∞–ø—Ä–∏–º–µ—Ä', '–º–∞–ª—ã –Ω–∞–ø—Ä–∏–º–µ—Ä —Ä–æ–¥–Ω–∞—è', '—Ü–µ–ª—å—é –Ω–∞–±—Ä–∞—Ç—å —Å—Ç–∞—Ç–µ–π', '–ø–æ–¥–∞–≤–ª—è—é—â–∞—è —á–∞—Å—Ç—å —Å—Ç–∞—Ç–µ–π', '–º–∏–ª–ª–∏–∞—Ä–¥–∞—Ö –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ —Å—Ç–∞—Ç–µ–π']\n",
      "1: ['apple newton messagepad', '–¥–æ–ø–æ–ª–Ω–µ–Ω–Ω–æ–π —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ google', '–ø—Ä–æ–¥—É–∫—Ç –∫–æ–º–ø–∞–Ω–∏–∏ google', '–æ—á–∫–∏ google glass', '—Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ google glassgoogle', '—Å–º–∞—Ä—Ç –æ—á–∫–∏ google', '—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞ apple newton', 'glass –ø—Ä–æ–¥—É–∫—Ç –∫–æ–º–ø–∞–Ω–∏–∏', '–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞ dvd –º–æ–≥–ª–∞', '–∞—Ç–º–æ—Å—Ñ–µ—Ä—É –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏', '–≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –∏–Ω–∂–µ–Ω–µ—Ä', '—Å—Ñ–µ—Ä–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏', '–≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ —Ä–µ—à–∏–ª–∞', '—à–ª–µ–º–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏', '–æ—á–∫–∏ –¥–æ–ø–æ–ª–Ω–µ–Ω–Ω–æ–π —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏']\n",
      "2: ['—á–∏—Ç–∞—Ç—å —Å–ª—É—à–∞—Ç—å –æ—Ç–≤–µ—Ç—ã', '–ø–æ–¥–∫–∞—Å—Ç–∞ —á–∏—Ç–∞—Ç—å —Å–ª—É—à–∞—Ç—å', '–∞–∫—É—Å—Ç–∏–∫–∏ —á–∏—Ç–∞—Ç—å —Å–ª—É—à–∞—Ç—å', '–∑–≤—É–∫–∞ —á–∏—Ç–∞—Ç—å —Å–ª—É—à–∞—Ç—å', '—á–∏—Ç–∞—Ç—å —Å–ª—É—à–∞—Ç—å –æ–±—Å—É–∂–¥–∞–µ–º', '—á–∏—Ç–∞—Ç—å —Å–ª—É—à–∞—Ç—å –ø—Ä–æ—Å—Ç—ã–º–∏', '—á–∏—Ç–∞—Ç—å —Å–ª—É—à–∞—Ç—å –≥–æ–≤–æ—Ä–∏–º', '—á–∏—Ç–∞—Ç—å —Å–ª—É—à–∞—Ç—å –∞–∫—É—Å—Ç–∏–∫–∞', '–∫–∞–±–µ–ª–∏ —á–∏—Ç–∞—Ç—å —Å–ª—É—à–∞—Ç—å', '–≤–æ–ø—Ä–æ—Å—ã —á–∏—Ç–∞—Ç—å —Å–ª—É—à–∞—Ç—å', '—á–∏—Ç–∞—Ç—å —Å–ª—É—à–∞—Ç—å', '—Å–ª—É—à–∞—Ç–µ–ª–µ–π —á–∏—Ç–∞—Ç—å —Å–ª—É—à–∞—Ç—å', '—á–∏—Ç–∞—Ç—å —Å–ª—É—à–∞—Ç—å –º–Ω–µ–Ω–∏–µ', '—á–∏—Ç–∞—Ç—å —Å–ª—É—à–∞—Ç—å –º—É–∑—ã–∫–∞–ª—å–Ω—ã–π', '—Å–∏—Ç—É–∞—Ü–∏—è —á–∏—Ç–∞—Ç—å —Å–ª—É—à–∞—Ç—å']\n",
      "3: ['—Ä–µ–≥—É–ª—è—Ä–Ω—ã–π –ø—Ä–∏–µ–º –ø–∞—Ä–∞—Ü–µ—Ç–∞–º–æ–ª–∞', '–∏–Ω—Ç–µ—Ä–≤—å—é –ø—Ä–æ—Ñ–µ—Å—Å–æ—Ä–∞ –∏—Ä–µ–Ω', '–ø–æ–ª–µ–∑–Ω–æ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–π –ø—Ä–∏–µ–º', '–ø—Ä–æ—Ñ–µ—Å—Å–æ—Ä–∞ –∏—Ä–µ–Ω –±—Ä–µ–π—Ç—É—ç–π—Ç', '–∏—Ä–µ–Ω –±—Ä–µ–π—Ç—É—ç–π—Ç –∑–∞–º–µ—Å—Ç–∏—Ç–µ–ª—è', '–∑–µ–ª–∞–Ω–¥–∏—è –∏—Ä–µ–Ω –±—Ä–µ–π—Ç—É—ç–π—Ç', '–∏–Ω—Ñ–µ–∫—Ü–∏–∏ —è–≤–ª—è–µ—Ç—Å—è –≤–∞–∫—Ü–∏–Ω–∞—Ü–∏—è', '–ø—Ä–∏–µ–º –∂–∞—Ä–æ–ø–æ–Ω–∏–∂–∞—é—â–∏—Ö —Å—Ä–µ–¥—Å—Ç–≤', '–≥—Ä–∏–ø–ø–æ–∑–Ω–æ–π –∏–Ω—Ñ–µ–∫—Ü–∏–∏ —è–≤–ª—è–µ—Ç—Å—è', '–±—Ä–µ–π—Ç—É—ç–π—Ç –∑–∞–º–µ—Å—Ç–∏—Ç–µ–ª—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∞', '–Ω–æ–≤–∞—è –∑–µ–ª–∞–Ω–¥–∏—è –∏—Ä–µ–Ω', '–≤–∑—Ä–æ—Å–ª—ã—Ö –ª—é–¥–µ–π –∏–Ω—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö', '—ç—Ñ—Ñ–µ–∫—Ç –¥–µ–π—Å—Ç–≤–∏—è –ø–∞—Ä–∞—Ü–µ—Ç–∞–º–æ–ª–∞', '–¥–µ–π—Å—Ç–≤–∏—è –ø–∞—Ä–∞—Ü–µ—Ç–∞–º–æ–ª–∞ –≥—Ä–∞–º–º', '–ª–µ—á–µ–Ω–∏—é –≤–Ω–µ–±–æ–ª—å–Ω–∏—á–Ω–æ–π –∏–Ω—Ñ–µ–∫—Ü–∏–∏']\n",
      "4: ['—Å–ª—É—á–∞–µ–≤ —Ä–∞–∫–∞ —Å–≤—è–∑–∞–Ω', '–ø—Ä–∏—á–∏–Ω–æ–π —Å–ª—É—á–∞–µ–≤ —Ä–∞–∫–∞', '–∏–∑–±–µ–∂–∞—Ç—å —Å–ª—É—á–∞–µ–≤ —Ä–∞–∫–∞', '–ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å —Å–ª—É—á–∞–µ–≤ —Ä–∞–∫–∞', '—Å–µ–º–∏ —Å–ª—É—á–∞–µ–≤ —Ä–∞–∫–∞', '—Ä–∞–∫–∞ —Ä–∞–∫–∞ –ª—ë–≥–∫–∏—Ö', '—Ä–∏—Å–∫ —Ä–∞–∫–∞ –≥—Ä—É–¥–∏', '—Ä–∞–∫–∞ –∫–æ–ª–æ—Ä–µ–∫—Ç–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∫–∞', '–∫–æ–ª–æ—Ä–µ–∫—Ç–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∫–∞ —Ä–∞–∫–∞', '—Å–ª—É—á–∞–µ–≤ —Ä–∞–∫–∞ –∞–Ω—É—Å–∞', '—Å–ª—É—á–∞–µ–≤ —Ä–∞–∫–∞ –ø–æ–ª–æ–≤–æ–≥–æ', '—Å–ª—É—á–∞–µ–≤ –≤–∞–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∫–∞', '—Å–ª—É—á–∞–µ–≤ –º–µ–ª–∞–Ω–æ–º—ã —Ä–∞–∫–∞', '—à–∞–Ω—Å –≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏—è —Ä–∞–∫–∞', '–∞–ª—å–±–µ—Ä—Ç–µ –≤—ã–∑—ã–≤–∞–µ—Ç —Å–ª—É—á–∞–µ–≤']\n",
      "5: ['–ø—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫–∞–Ω–∞–ª–∞', '–Ω–µ–¥–æ–≤–æ–ª–µ–Ω –ø—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å', '—Å—Ä–µ–¥–Ω—è—è –ø—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å', '–ø—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Å–µ—Ç–µ–≤–æ–≥–æ', '—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Å–µ—Ç–µ–≤–æ–≥–æ –ø–æ—Ä—Ç–∞', 'zero —Ñ–æ—Ç–æ raspberry', '–ø–æ—Ä—Ç–∞ –º–∞–ª–∏–Ω–∫–∏ –º–±–∏—Ç', '—Å–ª–æ–≤–∞–º —Å—Ä–µ–¥–Ω—è—è –ø—Ä–æ–ø—É—Å–∫–Ω–∞—è', '—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫–∞–Ω–∞–ª–∞ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç–æ–∫–æ–ª–æ', '—Å–æ–æ–±—â–µ–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–µ hey', '–Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Ä–µ–≥–∏–æ–Ω–∞—Ö —Å–∏—Ç—É–∞—Ü–∏—è', '—Å—Ç–æ–ª–∏—Ü–µ —Å—à–∞ –≤–∞—à–∏–Ω–≥—Ç–æ–Ω–µ', '–∂–∏—Ç–µ–ª—å –≤–∞—à–∏–Ω–≥—Ç–æ–Ω–∞ –∏–∑–≤–µ—Å—Ç–µ–Ω', '—Ä–µ—à–∏–ª –Ω–∞–ª–∞–¥–∏—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é', '–Ω–∞–ª–∞–¥–∏—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –æ—Ç–ø—Ä–∞–≤–∫—É']\n",
      "6: ['–ø–∞—Ä–∞–±–æ–ª–∏—á–µ—Å–∫–∏—Ö –∑–µ—Ä–∫–∞–ª —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω—ã', '—Å—Ç–æ–∏–º–æ—Å—Ç—å –º–ª–Ω –µ–≤—Ä–æ', 'noor iii –º–≤—Ç', '–º–µ—Ç—Ä–æ–≤—ã—Ö –ø–∞—Ä–∞–±–æ–ª–∏—á–µ—Å–∫–∏—Ö –∑–µ—Ä–∫–∞–ª', 'iii –º–≤—Ç –º–ª–Ω', '–º–≤—Ç –º–ª–Ω –µ–≤—Ä–æ', '—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –ø—Ä–æ–µ–∫—Ç–∞ noor', '–±–æ–ª—å—à–∏—Ö –º–µ—Ç—Ä–æ–≤—ã—Ö –ø–∞—Ä–∞–±–æ–ª–∏—á–µ—Å–∫–∏—Ö', 'csp —Å—Ç–∞–Ω—Ü–∏–∏ noor', '—ç–Ω–µ—Ä–≥–∏–∏ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ noor', '–º–≤—Ç —Å—Ç–æ–∏–º–æ—Å—Ç—å –º–ª–Ω', '–ø—É—Å—Ç—ã–Ω–µ —Å–∞—Ö–∞—Ä–∞ —Å–æ—Å—Ç–æ—è–ª–∞—Å—å—Ü–µ—Ä–µ–º–æ–Ω–∏—è', '—Å–∞—Ö–∞—Ä–∞ —Å–æ—Å—Ç–æ—è–ª–∞—Å—å—Ü–µ—Ä–µ–º–æ–Ω–∏—è —Ç–æ—Ä–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ', '—Å–æ—Å—Ç–æ—è–ª–∞—Å—å—Ü–µ—Ä–µ–º–æ–Ω–∏—è —Ç–æ—Ä–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –æ—Ç–∫—Ä—ã—Ç–∏—è–∫—Ä—É–ø–Ω–µ–π—à–µ–π', 'concentrating solar power']\n",
      "7: ['–≥–µ–Ω–æ–º—ã –∫–ª–æ–ø–æ–≤ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö', '—Ä–µ—à–µ–Ω–∏–µ —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞ –≥–µ–Ω–æ–º–∞', '—Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞ –≥–µ–Ω–æ–º–∞ –∫–ª–æ–ø–∞', '—Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞ –≥–µ–Ω–æ–º–∞ —Ç–∞–∫–æ–≥–æ', '–ø—Ä–∏—à–ª–æ—Å—å —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å –≥–µ–Ω–æ–º—ã', '—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —É–¥–∏–≤–ª—è—é—Ç –≥–µ–Ω–æ–º—ã', '–Ω–∞–ø—Ä–∏–º–µ—Ä –æ—Ç–ª–∏—á–∞—é—Ç—Å—è –≥–µ–Ω–æ–º—ã', '—Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å –≥–µ–Ω–æ–º—ã –∫–ª–æ–ø–æ–≤', '–≥–µ–Ω–æ–º—ã –∫–ª–æ–ø–æ–≤ –Ω–∞—á–∏–Ω–∞—è', '—É–¥–∏–≤–ª—è—é—Ç –≥–µ–Ω–æ–º—ã –∫–ª–æ–ø–æ–≤', '–≥–µ–Ω–æ–º—ã –∫–ª–æ–ø–æ–≤ —Ä–∞–∑–ª–∏—á–∞—é—Ç—Å—è', '–æ—Ç–ª–∏—á–∞—é—Ç—Å—è –≥–µ–Ω–æ–º—ã –∫–ª–æ–ø–æ–≤', '—Ñ–æ—Ç–æ brian kersey', '–æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –±–æ—Ä—å–±—ã', '–≥–µ–Ω–æ–º–∞ —Ç–∞–∫–æ–≥–æ –æ—Ä–≥–∞–Ω–∏–∑–º–∞']\n",
      "8: ['usb –¥–∏—Å–∫–æ–≤–æ–¥–µ usb', '—É–º–µ–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å usb', 'dos –∏–≥—Ä –Ω–∞–∫–æ–ø–∏—Ç–µ–ª—å', 'usb –∫–ª–∞–≤–∏–∞—Ç—É—Ä–∞ –∏—Ç–∞–∫', '–¥–∏—Å–∫–æ–≤–æ–¥–µ usb –∫–ª–∞–≤–∏–∞—Ç—É—Ä–∞', '—Ä–∞–±–æ—Ç–∞—Ç—å usb –∫–ª–∞–≤–∏–∞—Ç—É—Ä–∞', 'usb —Ñ–ª–æ–ø–ø–∏ –¥–∏—Å–∫–æ–≤–æ–¥–µ', '—Å—Ç–∞—Ä—ã—Ö dos –∏–≥—Ä', '–∏–∑–≤–µ—Å—Ç–Ω—ã–µ –∏–≥—Ä—ã –Ω–∞–ø—Ä–∏–º–µ—Ä', 'dos —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∑–∏–ª—Å—è', '—Ü–µ–ª—å—é –Ω–∞—à–µ–≥–æ –ø—É—Ç–µ—à–µ—Å—Ç–≤–∏—è', '—Ä–∞—Ä–∏—Ç–µ—Ç–Ω—ã–π –∫–æ–º–ø—å—é—Ç–µ—Ä –ø–æ—ç—Ç–æ–º—É', '—Ñ–ª–µ—à–∫–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–æ—Å—Ç–∞–≤–∏—Ç—å', '–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–ª–∞–±—ã–π –∫–æ–º–ø—å—é—Ç–µ—Ä', '—Å–∞–º—ã–µ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –∏–≥—Ä—ã']\n",
      "9: ['–æ–±–ª–∞–∫–µ –≥–∞–∑–∞ –ø—É—Å–∫–∞–π', '–æ–±–ª–∞–∫–µ –¥–æ–ª–∂–Ω—ã –Ω–∞—Ö–æ–¥–∏—Ç—å—Å—è', '–æ—Ç–∫—É–¥–∞ –≤—ã–±—Ä–æ—à–µ–Ω–æ –æ–±–ª–∞–∫–æ', '–≥–∞–ª–∞–∫—Ç–∏–∫–∏ –æ—Ç–∫—É–¥–∞ –≤—ã–±—Ä–æ—à–µ–Ω–æ', '–≤–µ—Ä–Ω–µ—Ç—Å—è –æ–±–ª–∞–∫–æ —Å–º–∏—Ç', '–æ–±–ª–∞–∫–æ —è–≤–ª—è–µ—Ç—Å—è –ø—Ä–∏–º–µ—Ä–æ–º', '–¥–∏—Å–∫–µ –≥–∞–ª–∞–∫—Ç–∏–∫–∏ –æ—Ç–∫—É–¥–∞', '–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å—Å—è –ø—Ä–∏–º–µ—Ä–Ω–æ –º–ª–Ω', '–ø—Ä–∏–º–µ—Ä–Ω–æ –º–ª–Ω –Ω–æ–≤—ã—Ö', '–Ω–∞–∑–≤–∞–Ω–Ω–æ–µ –æ–±–ª–∞–∫–æ–º —Å–º–∏—Ç', '–ø–µ—Ä–≤–æ–æ—Ç–∫—Ä—ã–≤–∞—Ç–µ–ª—å–Ω–∏—Ü—ã –≥–µ–π–ª—ã —Å–º–∏—Ç', '–≥–µ–π–ª—ã —Å–º–∏—Ç –æ–±–Ω–∞—Ä—É–∂–∏–≤—à–µ–π', '—Å–º–∏—Ç –æ–±–Ω–∞—Ä—É–∂–∏–≤—à–µ–π –æ–±—ä–µ–∫—Ç', '—Å–º–∏—Ç –∑–∞—Å–µ–∫–ª–∞ —Ä–∞–¥–∏–æ–≤–æ–ª–Ω—ã', '–≥–ª–∞–∑–∞ —á–µ–ª–æ–≤–µ–∫–∞ –æ–±–ª–∞–∫–æ']\n",
      "10: ['—Ä–∞–Ω–Ω–µ–π —Å—Ç–∞–¥–∏–∏ —Å–∏–≥–Ω–∞–ª—å–Ω–∞—è', '–ø—è—Ç–∏ —Ä–∞–∑–Ω—ã—Ö –≤–∏–¥–æ–≤', '–ø—è—Ç–∏ —Ä–∞–∑–Ω—ã—Ö –≤–∏–¥–∞—Ö', '—Ä–∞–∑–Ω—ã—Ö –≤–∏–¥–æ–≤ —Ä–∞–∫–∞', '—Å—Ç–∞–¥–∏–∏ —Å–∏–≥–Ω–∞–ª—å–Ω–∞—è –≥–µ–Ω–æ–º–Ω–∞—è', '—Ä–∞–∑–Ω—ã—Ö –≤–∏–¥–∞—Ö —Ä–∞–∫–∞', '–≤—ã—è–≤–∏–ª–∏ –≥–µ–Ω–æ–º–Ω—É—é —Å–∏–≥–Ω–∞—Ç—É—Ä—É', '–Ω–∞—à–ª–∏ –º–µ—Ç–∏–ª–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏–≥–Ω–∞—Ç—É—Ä—É', '–≤—ã—è–≤–∏–ª–∏ —É–Ω–∏–∫–∞–ª—å–Ω—É—é —Å–∏–≥–Ω–∞—Ç—É—Ä—É', '–±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –≤–∏–¥–æ–≤', '–∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –≤–∏–¥–æ–≤ —Ä–∞–∫–∞', '–≤–∏–¥–æ–≤ —Ä–∞–∫–∞ –≤–æ—Å—Ö–∏—â–µ–Ω–∏–µ', '–∫—Ä–æ–≤–∏ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å —Å—Ä–∞–∑—É', '–∞–Ω–∞–ª–∏–∑–æ–º –∫—Ä–æ–≤–∏ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å', '–±–∏–æ–ª–æ–≥ –ª–∞—É—Ä–∞ –µ–ª—å–Ω–∏—Ç—Å–∫–∏']\n",
      "11: ['—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º –¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç–æ–º alphabet', '–º–ª—Ä–¥ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –º–ª—Ä–¥', '–¥–æ—Ö–æ–¥–∞ alphabet –º–ª—Ä–¥', 'alphabet –º–ª—Ä–¥ –æ–±–µ—Å–ø–µ—á–∏–ª–∏', '–∫–æ–º–ø–∞–Ω–∏—è google alphabet', 'alphabet —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–æ–º–ø–∞–Ω–∏–∏', 'google alphabet –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏–ª–∞', '–∫–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–æ–º–ø–∞–Ω–∏–π apple', '–¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç–æ–º alphabet —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã', '—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã google', '–ø–æ–¥—Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º alphabet google', '—Å–µ—Ä–≤–∏—Å–∞ google alphabet', '–º–ª—Ä–¥ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ –≤—ã—Ä—É—á–∫–∞', 'alphabet –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏–ª–∞ –∫–≤–∞—Ä—Ç–∞–ª—å–Ω—ã–π', 'google –æ–±–≥–æ–Ω—è–ª–∞ apple']\n",
      "12: ['–∫–∞–º–µ—Ä–æ–π mp–∑–∞ –±–µ—Å–ø–ª–∞—Ç–Ω–æ', 'geektimes –ø—Ä–∏—è—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞', 'mp–∑–∞ –±–µ—Å–ø–ª–∞—Ç–Ω–æ –¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è', '—Ä–æ—Å—Å–∏—é —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∞–∫–∫—É–º—É–ª—è—Ç–æ—Ä', 'cm–≤–µ—Å –≥—Ä–º–∞—Ç–µ—Ä–∏–∞–ª –ø–ª–∞—Å—Ç–∏–∫–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ', '–≥—Ä–º–∞—Ç–µ—Ä–∏–∞–ª –ø–ª–∞—Å—Ç–∏–∫–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–∞–ª–æ–≤', '–ø–ª–∞—Å—Ç–∏–∫–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–∞–ª–æ–≤ —á–∞—Å—Ç–æ—Ç–∞', 'ghz–¥–∞–ª—å–Ω–æ—Å—Ç—å —Å–≤—è–∑–∏ –º–µ—Ç—Ä–æ–≤–∫–∞–º–µ—Ä–∞', '–∫–æ–º–ø–ª–µ–∫—Ç–∞—Ü–∏–∏ –∫–æ–º–ø–ª–µ–∫—Ç–∞—Ü–∏—è –ø–æ—Å—Ç–∞–≤–∫–∏', 'usb –∑–∞—Ä—è–¥–∫–∞ –∫–∞–±–µ–ª—å', '–ª—é–±—ã–µ –ø—Ä–µ–ø—è—Ç—Å—Ç–≤–∏—è —Å–æ–µ–¥–∏–Ω—è—è', '–≤—ã–±–æ—Ä–∞ —Å–ª–æ–∂–Ω–æ–π —Ç–µ—Ö–Ω–∏–∫–∏dronk', '—Å—Ç–æ–∏–º–æ—Å—Ç–∏ –ø–æ–∫—É–ø–∫–∏ –ø—Ä–∏–æ–±—Ä–µ—Ç–∞—è', '–ø–æ–∫—É–ø–∫–∏ –ø—Ä–∏–æ–±—Ä–µ—Ç–∞—è —Ç–æ–≤–∞—Ä', '—Å—Å—ã–ª–∫–∞–º –ø–æ–¥—Ä–æ–±–Ω–µ–µ dronk']\n",
      "13: ['–∫–∞—á–µ—Å—Ç–≤–µ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞', '–∫–∞—á–µ—Å—Ç–≤–µ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è', '—Ñ—É–Ω–∫—Ü–∏—è –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏', '—Ç–∞–∫–æ–π—à–ª–µ–º –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏', '—Å–æ–≤—Å–µ–ºgoogle cardboard –Ω–æ–≤–æ–µ', '–≤–µ—Ä—Å–∏—è google cardboard', '–∫–∞—Å–∞–µ—Ç—Å—è google cardboard', '–≤–∏—Ä—Ç—É–∞–ª—å–Ω—É—é —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—å google', 'google cardboard —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ', '–∫–æ—Ä–ø–æ—Ä–∞—Ü–∏—è –ø–ª–∞–Ω–∏—Ä—É–µ—Ç –≤—ã–ø—É—Å—Ç–∏—Ç—å', '–ø–ª–∞–Ω–∏—Ä—É–µ—Ç –≤—ã–ø—É—Å—Ç–∏—Ç—å –¥–µ–≤–∞–π—Å', '–≤—ã–ø—É—Å—Ç–∏—Ç—å –æ—á–µ—Ä–µ–¥–Ω—É—é –º–æ–¥–µ–ª—å', '–ª–∏–Ω–∑—ã –ø–ª—é—Å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è', '–æ—á–µ—Ä–µ–¥–Ω—É—é –º–æ–¥–µ–ª—å –≤–∏–¥–µ–æ–æ—á–∫–æ–≤', '–ø–∏—à–µ—Çbusinessinsider –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏—è –ø–ª–∞–Ω–∏—Ä—É–µ—Ç']\n",
      "14: ['—Å–∞–º–æ–ª—ë—Ç–∞–º –∏–ª–æ–Ω –º–∞—Å–∫', '–∏—Å—Ç–æ—á–Ω–∏–∫ –ø–∏—Ç–∞–Ω–∏—è –æ–∫–∞–∑–∞–ª—Å—è', '–≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –ø–∏—Ç–∞–Ω–∏—è', '–ø–æ–ª–Ω–æ—Å—Ç—å—é —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∏–π —Å–∞–º–æ–ª—ë—Ç', '–º–∞—Å–∫ —Ö–æ—á–µ—Ç —Å–æ–∑–¥–∞—Ç—å', '–ø–∏—Ç–∞–Ω–∏—è –æ–∫–∞–∑–∞–ª—Å—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ', '–ª–∏—Ç–∏–π –∏–æ–Ω–Ω—ã–µ –∞–∫–∫—É–º—É–ª—è—Ç–æ—Ä—ã', '—Ö–æ—á–µ—Ç –¥–µ–ª–∞—Ç—å —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∏–µ', '—É–ø–æ–º–∏–Ω–∞–µ—Ç —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∏–µ —Å–∞–º–æ–ª—ë—Ç—ã', '—ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∏–µ —Å–∞–º–æ–ª—ë—Ç—ã –≤—Å–ø–æ–º–∏–Ω–∞—é—Ç–∂—É—Ä–Ω–∞–ª–∏—Å—Ç—ã', '—Å–∞–º–æ–ª—ë—Ç–æ–≤ –ø—ã—Ç–∞–ª–∏—Å—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å', '–¥–µ–ª–∞—Ç—å —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∏–µ –¥–≤–∏–≥–∞—Ç–µ–ª–∏', '–ª–µ—Ç–æ–º —Ä–µ–∞–ª—å–Ω–æ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å', '–ø—Ä–æ–µ–∫—Ç —Ä–µ–∞–ª—å–Ω–æ –Ω–∞—á–Ω—É—Ç', '—Ä–µ–∞–ª—å–Ω–æ –Ω–∞—á–Ω—É—Ç –≤–æ–æ–±—â–µ']\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T05:56:29.955850Z",
     "start_time": "2025-11-20T20:49:30.777829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import jaccard_score, hamming_loss, f1_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, OneHotEncoder\n",
    "from scipy.sparse import hstack \n",
    "import time\n",
    "import numpy as np\n",
    "import joblib  \n",
    "\n",
    "df = pd.read_pickle(\"data/data_with_main_info.pkl\")\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['hubs'])\n",
    "valid_hub_mask = y.sum(axis=0) >= 2  \n",
    "y_filtered = y[:, valid_hub_mask]\n",
    "\n",
    "filtered_hubs = [\n",
    "    [hub for hub in hubs if hub in mlb.classes_[valid_hub_mask]]\n",
    "    for hubs in df['hubs']\n",
    "]\n",
    "mlb_filtered = MultiLabelBinarizer()\n",
    "y = mlb_filtered.fit_transform(filtered_hubs)\n",
    "\n",
    "non_empty = y.sum(axis=1) > 0\n",
    "df = df[non_empty].copy()\n",
    "y = y[non_empty]\n",
    "\n",
    "print(f\"–°—Ç–∞—Ç–µ–π –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(df)}, —Ö–∞–±–æ–≤: {y.shape[1]}\")\n",
    "\n",
    "print(\"–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è...\")\n",
    "start_time = time.time()\n",
    "\n",
    "df['text_main_str'] = df['text_main'].apply(lambda x: ' '.join(x) if x else '')\n",
    "df['cleaned_keywords_str'] = df['cleaned_keywords'].fillna('').astype(str)\n",
    "df['full_text_with_yake'] = (\n",
    "    df['cleaned_title'].fillna('') + ' ' +\n",
    "    df['cleaned_keywords_str'] + ' ' +\n",
    "    df['text_main_str']\n",
    ")\n",
    "\n",
    "mask = df['full_text_with_yake'].str.len() > 50\n",
    "df = df[mask]\n",
    "y = y[mask.values] \n",
    "\n",
    "df['datetime'] = pd.to_datetime(df['time'])\n",
    "df['year'] = df['datetime'].dt.year\n",
    "df['month'] = df['datetime'].dt.month\n",
    "df['dayofweek'] = df['datetime'].dt.dayofweek\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "\n",
    "time_features = ['year', 'month', 'dayofweek', 'hour']\n",
    "time_encoder = OneHotEncoder(sparse_output=True, drop='first')\n",
    "X_time = time_encoder.fit_transform(df[time_features])\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=40000,           \n",
    "    ngram_range=(1, 3),           \n",
    "    min_df=2,                     \n",
    "    max_df=0.8,                \n",
    "    stop_words=[\n",
    "        '–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ',\n",
    "        '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫', '–µ–≥–æ', '–Ω–æ', '–¥–∞'\n",
    "    ],\n",
    "    sublinear_tf=True,\n",
    "    lowercase=True,\n",
    "    smooth_idf=True,\n",
    "    norm='l2',\n",
    "    use_idf=True,\n",
    "    binary=False\n",
    ")\n",
    "X_text = vectorizer.fit_transform(df['full_text_with_yake'])\n",
    "\n",
    "username_vectorizer = TfidfVectorizer(\n",
    "    analyzer='char_wb',\n",
    "    ngram_range=(2, 4),\n",
    "    min_df=1,\n",
    "    max_features=1500,\n",
    "    sublinear_tf=True,\n",
    "    lowercase=True\n",
    ")\n",
    "X_username = username_vectorizer.fit_transform(df['username'].fillna(''))\n",
    "\n",
    "X = hstack([X_text, X_username, X_time])\n",
    "\n",
    "print(f\"!–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∑–∞ {time.time() - start_time:.2f} —Å–µ–∫, X: {X.shape}, y: {y.shape}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...\")\n",
    "modelFull = MultiOutputClassifier(\n",
    "    LogisticRegression(\n",
    "        max_iter=2000,\n",
    "        class_weight='balanced',\n",
    "        C=0.5,\n",
    "        solver='liblinear',\n",
    "        random_state=42,\n",
    "        tol=1e-4,\n",
    "        fit_intercept=True\n",
    "    ),\n",
    "    n_jobs=-1\n",
    ")\n",
    "modelFull.fit(X_train, y_train)\n",
    "\n",
    "y_pred = modelFull.predict(X_test)\n",
    "jaccard = jaccard_score(y_test, y_pred, average='samples')\n",
    "hamming = hamming_loss(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:\")\n",
    "print(f\"Jaccard Score: {jaccard:.4f}\")\n",
    "print(f\"Hamming Loss:  {hamming:.4f}\")\n",
    "print(f\"–ö–ª–∞—Å—Å–æ–≤: {y.shape[1]}, —Å—Ç–∞—Ç–µ–π: {len(df)}\")\n",
    "\n",
    "joblib.dump(vectorizer, 'vectorizer.pkl')\n",
    "joblib.dump(username_vectorizer, 'username_vectorizer.pkl')\n",
    "joblib.dump(time_encoder, 'time_encoder.pkl')\n",
    "joblib.dump(mlb_filtered, 'mlb_filtered.pkl')\n",
    "joblib.dump(modelFull, 'helpers/modelFull.pkl')  \n"
   ],
   "id": "bbcb15e359845800",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°—Ç–∞—Ç–µ–π –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: 98064, —Ö–∞–±–æ–≤: 953\n",
      "–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è...\n",
      "!–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∑–∞ 43.58 —Å–µ–∫, X: (97731, 41549), y: (97731, 953)\n",
      "–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...\n",
      "\n",
      "‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:\n",
      "Jaccard Score: 0.3700\n",
      "Hamming Loss:  0.0060\n",
      "–ö–ª–∞—Å—Å–æ–≤: 953, —Å—Ç–∞—Ç–µ–π: 97731\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['modelFull.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T19:22:15.037250Z",
     "start_time": "2025-12-02T18:44:34.673561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_predict\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import jaccard_score, hamming_loss, f1_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, OneHotEncoder, StandardScaler\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import time\n",
    "import numpy as np\n",
    "import joblib\n",
    "from collections import Counter\n",
    "\n",
    "df = pd.read_pickle(\"data/data_with_main_info.pkl\")\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['hubs'])\n",
    "\n",
    "min_samples_per_class = max(3, int(len(df) * 0.001))  \n",
    "valid_hub_mask = y.sum(axis=0) >= min_samples_per_class\n",
    "y_filtered = y[:, valid_hub_mask]\n",
    "\n",
    "filtered_hubs = [\n",
    "    [hub for hub in hubs if hub in mlb.classes_[valid_hub_mask]]\n",
    "    for hubs in df['hubs']\n",
    "]\n",
    "mlb_filtered = MultiLabelBinarizer()\n",
    "y = mlb_filtered.fit_transform(filtered_hubs)\n",
    "\n",
    "non_empty = y.sum(axis=1) > 0\n",
    "df = df[non_empty].copy()\n",
    "y = y[non_empty]\n",
    "\n",
    "print(f\"–°—Ç–∞—Ç–µ–π –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(df)}, —Ö–∞–±–æ–≤: {y.shape[1]}\")\n",
    "print(f\"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ö–∞–±–æ–≤: {dict(Counter(y.sum(axis=1)).most_common(10))}\")\n",
    "\n",
    "print(\"\\n–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è...\")\n",
    "start_time = time.time()\n",
    "\n",
    "df['text_main_str'] = df['text_main'].apply(lambda x: ' '.join(x) if x else '')\n",
    "df['cleaned_keywords_str'] = df['cleaned_keywords'].fillna('').astype(str)\n",
    "\n",
    "df['full_text_weighted'] = (\n",
    "    df['cleaned_title'].fillna('') + ' ' +\n",
    "    df['cleaned_title'].fillna('') + ' ' +\n",
    "    df['cleaned_keywords_str'] + ' ' +\n",
    "    df['text_main_str']\n",
    ")\n",
    "\n",
    "min_text_length = 50\n",
    "mask = df['full_text_weighted'].str.len() > min_text_length\n",
    "df = df[mask]\n",
    "y = y[mask.values]\n",
    "\n",
    "df['datetime'] = pd.to_datetime(df['time'])\n",
    "df['year'] = df['datetime'].dt.year\n",
    "df['month'] = df['datetime'].dt.month\n",
    "df['dayofweek'] = df['datetime'].dt.dayofweek\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['is_weekend'] = df['dayofweek'].isin([5, 6]).astype(int)\n",
    "df['quarter'] = df['datetime'].dt.quarter\n",
    "\n",
    "time_features = ['year', 'month', 'dayofweek', 'hour', 'is_weekend', 'quarter']\n",
    "time_encoder = OneHotEncoder(sparse_output=True, drop='first')\n",
    "X_time = time_encoder.fit_transform(df[time_features])\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=30000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.8,\n",
    "    stop_words=[\n",
    "        '–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ',\n",
    "        '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫', '–µ–≥–æ', '–Ω–æ', '–¥–∞',\n",
    "        '—Ç—ã', '–ø–æ', '–Ω–æ', '–∑–∞', '–∏–∑', '—ç—Ç–æ', '–∏–ª–∏', '—É', '–∂–µ', '–±—ã',\n",
    "        '–≤–æ—Ç', '–æ—Ç', '–º–µ–Ω—è', '–µ–º—É', '–Ω–µ—Ç', '–æ', '–µ—â–µ', '–∫–æ–≥–¥–∞',\n",
    "        '–¥–∞–∂–µ', '–Ω—É', '–ª–∏', '–µ—Å–ª–∏', '–±—ã–ª', '–¥–æ', '–Ω–∏', '–±—ã—Ç—å',\n",
    "        '–ø—Ä–∏', '—Ç–∞–∫–∂–µ', '–∫', '–ø–æ', '–Ω–∞', '—ç—Ç–æ—Ç', '—á—Ç–æ', '–∫–æ—Ç–æ—Ä—ã–π'\n",
    "    ],\n",
    "    sublinear_tf=True,\n",
    "    lowercase=True,\n",
    "    smooth_idf=True,\n",
    "    norm='l2',\n",
    "    use_idf=True,\n",
    "    binary=False,\n",
    "    analyzer='word'\n",
    ")\n",
    "\n",
    "X_text = vectorizer.fit_transform(df['full_text_weighted'])\n",
    "\n",
    "username_vectorizer = TfidfVectorizer(\n",
    "    analyzer='char',\n",
    "    ngram_range=(3, 4),\n",
    "    min_df=2,\n",
    "    max_features=1500,\n",
    "    sublinear_tf=True,\n",
    "    lowercase=True,\n",
    "    binary=True\n",
    ")\n",
    "X_username = username_vectorizer.fit_transform(df['username'].fillna(''))\n",
    "\n",
    "df['text_length'] = df['full_text_weighted'].str.len()\n",
    "df['word_count'] = df['full_text_weighted'].str.split().str.len()\n",
    "\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "X_numeric = csr_matrix(scaler.fit_transform(df[['text_length', 'word_count']]))\n",
    "\n",
    "X = hstack([X_text, X_username, X_time, X_numeric])\n",
    "print(f\"–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∑–∞ {time.time() - start_time:.2f} —Å–µ–∫, X: {X.shape}, y: {y.shape}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\n–†–∞–∑–º–µ—Ä train: {X_train.shape}, test: {X_test.shape}\")\n",
    "\n",
    "print(\"\\n–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...\")\n",
    "model = MultiOutputClassifier(\n",
    "    LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        class_weight='balanced',\n",
    "        C=0.8,\n",
    "        solver='liblinear',\n",
    "        penalty='l2',\n",
    "        random_state=42,\n",
    "        tol=1e-4,\n",
    "        fit_intercept=True\n",
    "    ),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"–ö–†–û–°–°-–í–ê–õ–ò–î–ê–¶–ò–Ø (5 —Ñ–æ–ª–¥–æ–≤)\")\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fold_jaccard_scores = []\n",
    "fold_f1_micro_scores = []\n",
    "fold_f1_macro_scores = []\n",
    "fold_hamming_losses = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train), 1):\n",
    "    print(f\"\\n–§–æ–ª–¥ {fold}/5:\")\n",
    "    \n",
    "    X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "    \n",
    "    model_fold = MultiOutputClassifier(\n",
    "        LogisticRegression(\n",
    "            max_iter=1000,\n",
    "            class_weight='balanced',\n",
    "            C=0.8,\n",
    "            solver='liblinear',\n",
    "            random_state=42 + fold\n",
    "        ),\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    model_fold.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    y_pred_fold = model_fold.predict(X_val_fold)\n",
    "    \n",
    "    jaccard_fold = jaccard_score(y_val_fold, y_pred_fold, average='samples')\n",
    "    f1_micro_fold = f1_score(y_val_fold, y_pred_fold, average='micro')\n",
    "    f1_macro_fold = f1_score(y_val_fold, y_pred_fold, average='macro')\n",
    "    hamming_fold = hamming_loss(y_val_fold, y_pred_fold)\n",
    "    \n",
    "    fold_jaccard_scores.append(jaccard_fold)\n",
    "    fold_f1_micro_scores.append(f1_micro_fold)\n",
    "    fold_f1_macro_scores.append(f1_macro_fold)\n",
    "    fold_hamming_losses.append(hamming_fold)\n",
    "    \n",
    "    print(f\"  Jaccard: {jaccard_fold:.4f}, F1 Micro: {f1_micro_fold:.4f}, Hamming Loss: {hamming_fold:.4f}\")\n",
    "\n",
    "print(\"–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ö–†–û–°–°-–í–ê–õ–ò–î–ê–¶–ò–ò\")\n",
    "\n",
    "print(f\"Jaccard Score (samples):\")\n",
    "print(f\"  –§–æ–ª–¥—ã: {[f'{v:.4f}' for v in fold_jaccard_scores]}\")\n",
    "print(f\"  –°—Ä–µ–¥–Ω–µ–µ: {np.mean(fold_jaccard_scores):.4f} (¬±{np.std(fold_jaccard_scores):.4f})\")\n",
    "\n",
    "print(f\"\\nF1 Micro:\")\n",
    "print(f\"  –§–æ–ª–¥—ã: {[f'{v:.4f}' for v in fold_f1_micro_scores]}\")\n",
    "print(f\"  –°—Ä–µ–¥–Ω–µ–µ: {np.mean(fold_f1_micro_scores):.4f} (¬±{np.std(fold_f1_micro_scores):.4f})\")\n",
    "\n",
    "print(f\"\\nF1 Macro:\")\n",
    "print(f\"  –§–æ–ª–¥—ã: {[f'{v:.4f}' for v in fold_f1_macro_scores]}\")\n",
    "print(f\"  –°—Ä–µ–¥–Ω–µ–µ: {np.mean(fold_f1_macro_scores):.4f} (¬±{np.std(fold_f1_macro_scores):.4f})\")\n",
    "\n",
    "print(f\"\\nHamming Loss:\")\n",
    "print(f\"  –§–æ–ª–¥—ã: {[f'{v:.4f}' for v in fold_hamming_losses]}\")\n",
    "print(f\"  –°—Ä–µ–¥–Ω–µ–µ: {np.mean(fold_hamming_losses):.4f} (¬±{np.std(fold_hamming_losses):.4f})\")\n",
    "\n",
    "print(\"–û–ë–£–ß–ï–ù–ò–ï –§–ò–ù–ê–õ–¨–ù–û–ô –ú–û–î–ï–õ–ò\")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"–û–¶–ï–ù–ö–ê –ù–ê –¢–ï–°–¢–û–í–´–• –î–ê–ù–ù–´–•\")\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "\n",
    "jaccard = jaccard_score(y_test, y_pred, average='samples')\n",
    "hamming = hamming_loss(y_test, y_pred)\n",
    "f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "precision_micro = precision_score(y_test, y_pred, average='micro')\n",
    "recall_micro = recall_score(y_test, y_pred, average='micro')\n",
    "\n",
    "print(f\"\\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö:\")\n",
    "print(f\"Jaccard Score (samples): {jaccard:.4f}\")\n",
    "print(f\"Hamming Loss: {hamming:.4f}\")\n",
    "print(f\"F1 Micro: {f1_micro:.4f}\")\n",
    "print(f\"F1 Macro: {f1_macro:.4f}\")\n",
    "print(f\"Precision Micro: {precision_micro:.4f}\")\n",
    "print(f\"Recall Micro: {recall_micro:.4f}\")\n",
    "print(f\"–ö–ª–∞—Å—Å–æ–≤: {y.shape[1]}, —Å—Ç–∞—Ç–µ–π: {len(df)}\")\n",
    "\n",
    "print(\"–≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ –° –†–ê–ó–ù–´–ú–ò –ü–û–†–û–ì–ê–ú–ò\")\n",
    "\n",
    "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "for threshold in thresholds:\n",
    "    y_pred_thresholded = np.array([(proba[:, 1] > threshold).astype(int) for proba in y_pred_proba]).T\n",
    "    jaccard_th = jaccard_score(y_test, y_pred_thresholded, average='samples')\n",
    "    print(f\"–ü–æ—Ä–æ–≥ {threshold}: Jaccard = {jaccard_th:.4f}\")\n",
    "\n",
    "print(\"–°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ò\")\n",
    "\n",
    "joblib.dump(vectorizer, 'actual_helpers/vectorizer_last.pkl')\n",
    "joblib.dump(username_vectorizer, 'actual_helpers/username_vectorizer_last.pkl')\n",
    "joblib.dump(time_encoder, 'actual_helpers/time_encoder_last.pkl')\n",
    "joblib.dump(mlb_filtered, 'actual_helpers/mlb_filtered_last.pkl')\n",
    "joblib.dump(model, 'model_last.pkl')\n",
    "joblib.dump(scaler, 'actual_helpers/scaler_last.pkl')\n",
    "\n",
    "print(f\"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏: Jaccard = {np.mean(fold_jaccard_scores):.4f} (¬±{np.std(fold_jaccard_scores):.4f})\")\n",
    "print(f\"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ç–µ—Å—Ç–µ: Jaccard = {jaccard:.4f}\")"
   ],
   "id": "fd4d7646c06affc7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°—Ç–∞—Ç–µ–π –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: 98061, —Ö–∞–±–æ–≤: 393\n",
      "–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ö–∞–±–æ–≤: {np.int64(3): 39006, np.int64(4): 29976, np.int64(5): 23474, np.int64(2): 5465, np.int64(1): 133, np.int64(6): 7}\n",
      "\n",
      "–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è...\n",
      "–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∑–∞ 33.22 —Å–µ–∫, X: (98035, 31555), y: (98035, 393)\n",
      "\n",
      "–†–∞–∑–º–µ—Ä train: (78428, 31555), test: (19607, 31555)\n",
      "\n",
      "–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...\n",
      "–ö–†–û–°–°-–í–ê–õ–ò–î–ê–¶–ò–Ø (5 —Ñ–æ–ª–¥–æ–≤)\n",
      "\n",
      "–§–æ–ª–¥ 1/5:\n",
      "  Jaccard: 0.3858, F1 Micro: 0.5206, Hamming Loss: 0.0125\n",
      "\n",
      "–§–æ–ª–¥ 2/5:\n",
      "  Jaccard: 0.3843, F1 Micro: 0.5196, Hamming Loss: 0.0125\n",
      "\n",
      "–§–æ–ª–¥ 3/5:\n",
      "  Jaccard: 0.3816, F1 Micro: 0.5154, Hamming Loss: 0.0126\n",
      "\n",
      "–§–æ–ª–¥ 4/5:\n",
      "  Jaccard: 0.3841, F1 Micro: 0.5191, Hamming Loss: 0.0126\n",
      "\n",
      "–§–æ–ª–¥ 5/5:\n",
      "  Jaccard: 0.3818, F1 Micro: 0.5170, Hamming Loss: 0.0126\n",
      "–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ö–†–û–°–°-–í–ê–õ–ò–î–ê–¶–ò–ò\n",
      "Jaccard Score (samples):\n",
      "  –§–æ–ª–¥—ã: ['0.3858', '0.3843', '0.3816', '0.3841', '0.3818']\n",
      "  –°—Ä–µ–¥–Ω–µ–µ: 0.3835 (¬±0.0016)\n",
      "\n",
      "F1 Micro:\n",
      "  –§–æ–ª–¥—ã: ['0.5206', '0.5196', '0.5154', '0.5191', '0.5170']\n",
      "  –°—Ä–µ–¥–Ω–µ–µ: 0.5183 (¬±0.0019)\n",
      "\n",
      "F1 Macro:\n",
      "  –§–æ–ª–¥—ã: ['0.5385', '0.5386', '0.5314', '0.5382', '0.5363']\n",
      "  –°—Ä–µ–¥–Ω–µ–µ: 0.5366 (¬±0.0028)\n",
      "\n",
      "Hamming Loss:\n",
      "  –§–æ–ª–¥—ã: ['0.0125', '0.0125', '0.0126', '0.0126', '0.0126']\n",
      "  –°—Ä–µ–¥–Ω–µ–µ: 0.0126 (¬±0.0001)\n",
      "–û–ë–£–ß–ï–ù–ò–ï –§–ò–ù–ê–õ–¨–ù–û–ô –ú–û–î–ï–õ–ò\n",
      "–û–¶–ï–ù–ö–ê –ù–ê –¢–ï–°–¢–û–í–´–• –î–ê–ù–ù–´–•\n",
      "\n",
      "‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö:\n",
      "Jaccard Score (samples): 0.3852\n",
      "Hamming Loss: 0.0128\n",
      "F1 Micro: 0.5188\n",
      "F1 Macro: 0.5400\n",
      "Precision Micro: 0.4031\n",
      "Recall Micro: 0.7274\n",
      "–ö–ª–∞—Å—Å–æ–≤: 393, —Å—Ç–∞—Ç–µ–π: 98035\n",
      "–≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ –° –†–ê–ó–ù–´–ú–ò –ü–û–†–û–ì–ê–ú–ò\n",
      "–ü–æ—Ä–æ–≥ 0.1: Jaccard = 0.1103\n",
      "–ü–æ—Ä–æ–≥ 0.2: Jaccard = 0.1965\n",
      "–ü–æ—Ä–æ–≥ 0.3: Jaccard = 0.2724\n",
      "–ü–æ—Ä–æ–≥ 0.4: Jaccard = 0.3350\n",
      "–ü–æ—Ä–æ–≥ 0.5: Jaccard = 0.3852\n",
      "–°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ò\n",
      "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏: Jaccard = 0.3835 (¬±0.0016)\n",
      "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ç–µ—Å—Ç–µ: Jaccard = 0.3852\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
