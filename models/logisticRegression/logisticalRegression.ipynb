{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "\n",
    "print(\"Загружаем данные...\")\n",
    "df = pd.read_csv('dataset/habr.csv', encoding='utf-8')\n",
    "print(\"Размер исходного датасета: {len(df)}\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "    text = re.sub(r'[^а-яа-яёёa-za-z\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def parse_hubs(hub_str):\n",
    "    if pd.isna(hub_str):\n",
    "        return []\n",
    "    try:\n",
    "        return ast.literal_eval(hub_str)\n",
    "    except:\n",
    "        if isinstance(hub_str, str):\n",
    "            return [hub.strip() for hub in hub_str.split(',')]\n",
    "        return []\n",
    "\n",
    "print(\"Очищаем тексты...\")\n",
    "df['hubs'] = df['hubs'].apply(parse_hubs)\n",
    "\n",
    "text_columns = ['title', 'keywords', 'text']\n",
    "for col in text_columns:\n",
    "    df[f'cleaned_{col}'] = pd.Series(tqdm(\n",
    "        (preprocess_text(text) for text in df[col]),\n",
    "        total=len(df),\n",
    "        desc=f\"Обработка {col}\"\n",
    "    ))\n",
    "\n",
    "df['full_text'] = (\n",
    "    df['cleaned_title'] + \" \" +\n",
    "    df['cleaned_keywords'].fillna(\"\") + \" \" +\n",
    "    df['cleaned_text']\n",
    ")\n",
    "\n",
    "df = df[df['full_text'].str.len() > 50]\n",
    "hub_counts = df['hubs'].explode().value_counts()\n",
    "valid_hubs = hub_counts[hub_counts >= 10].index\n",
    "df['hubs'] = df['hubs'].apply(lambda hubs: [h for h in hubs if h in valid_hubs])\n",
    "df = df[df['hubs'].map(len) >= 3]\n",
    "\n",
    "print(f\"Итоговый размер: {len(df)}\")\n",
    "\n",
    "df.to_pickle(\"processed_habr.pkl\")\n",
    "print(\"Данные сохранены в processed_habr.pkl\")\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T05:56:29.951758200Z",
     "start_time": "2025-11-18T20:53:53.489995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import jaccard_score, hamming_loss\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from scipy.sparse import vstack\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_pickle(\"data/processed_habr.pkl\")\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['hubs'])\n",
    "print(f\"Хабы: {len(mlb.classes_)}, Статьи: {df.shape[0]}\")\n",
    "\n",
    "min_hub_count = 2\n",
    "valid_hub_indices = y.sum(axis=0) >= min_hub_count\n",
    "y_filtered = y[:, valid_hub_indices]\n",
    "\n",
    "print(f\"Исходное число хабов: {y.shape[1]}\")\n",
    "print(f\"После фильтрации: {y_filtered.shape[1]} хабов\")\n",
    "\n",
    "mlb_filtered = MultiLabelBinarizer()\n",
    "filtered_hubs = []\n",
    "for hubs_list in df['hubs']:\n",
    "    filtered_hub = [hub for hub in hubs_list if hub in mlb.classes_[valid_hub_indices]]\n",
    "    filtered_hubs.append(filtered_hub)\n",
    "\n",
    "y = mlb_filtered.fit_transform(filtered_hubs)\n",
    "\n",
    "class_counts = y.sum(axis=0)\n",
    "print(f\"Минимальное количество примеров в классе: {class_counts.min()}\")\n",
    "print(f\"Максимальное количество примеров в классе: {class_counts.max()}\")\n",
    "\n",
    "non_empty_indices = y.sum(axis=1) > 0\n",
    "df = df[non_empty_indices]\n",
    "y = y[non_empty_indices]\n",
    "\n",
    "print(f\"Статей после удаления пустых: {len(df)}\")\n",
    "\n",
    "print(\"Векторизация...\")\n",
    "start_time = time.time()\n",
    "\n",
    "russian_stopwords = [\n",
    "    'и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со',\n",
    "    'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да'\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=3000,\n",
    "    ngram_range=(1, 1),\n",
    "    min_df=1,\n",
    "    max_df=0.95,\n",
    "    stop_words=russian_stopwords,\n",
    "    sublinear_tf=True,\n",
    "    lowercase=True,\n",
    "    smooth_idf=True\n",
    ")\n",
    "\n",
    "texts = df['full_text'].tolist()\n",
    "X = vectorizer.fit_transform(texts)  \n",
    "\n",
    "print(f\"\\nВекторизация завершена за {time.time() - start_time:.2f} сек\")\n",
    "print(f\"Размерность: {X.shape}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42  \n",
    ")\n",
    "\n",
    "print(f\"Обучающая выборка: {X_train.shape[0]}\")\n",
    "print(f\"Тестовая выборка: {X_test.shape[0]}\")\n",
    "\n",
    "print(\"Обучение модели...\")\n",
    "model = MultiOutputClassifier(\n",
    "    LogisticRegression(\n",
    "        max_iter=1000, \n",
    "        class_weight='balanced', \n",
    "        C=0.5, \n",
    "        solver='liblinear', \n",
    "        random_state=42\n",
    "    ),\n",
    "    n_jobs=-1\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "jaccard = jaccard_score(y_test, y_pred, average='samples')\n",
    "hamming = hamming_loss(y_test, y_pred)\n",
    "\n",
    "print(f\"Результаты:\")\n",
    "print(f\"Jaccard Score: {jaccard:.4f}\")\n",
    "print(f\"Hamming Loss: {hamming:.4f}\")\n",
    "\n",
    "print(f\"Детали:\")\n",
    "print(f\"Количество классов: {y.shape[1]}\")\n",
    "print(f\"Предсказано не-нулей: {(y_pred.sum(axis=1) > 0).sum()} / {y_pred.shape[0]}\")\n",
    "print(f\"Среднее количество хабов на статью: {y.sum(axis=1).mean():.2f}\")"
   ],
   "id": "c519d734e44064e1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Хабы: 955, Статьи: 98064\n",
      "Исходное число хабов: 955\n",
      "После фильтрации: 953 хабов\n",
      "Минимальное количество примеров в классе: 2\n",
      "Максимальное количество примеров в классе: 13906\n",
      "Статей после удаления пустых: 98064\n",
      "Векторизация...\n",
      "\n",
      "Векторизация завершена за 149.50 сек\n",
      "Размерность: (98064, 3000)\n",
      "Обучающая выборка: 78451\n",
      "Тестовая выборка: 19613\n",
      "Обучение модели...\n",
      "\n",
      "Результаты:\n",
      "Jaccard Score: 0.2451\n",
      "Hamming Loss: 0.0114\n",
      "\n",
      "Детали:\n",
      "Количество классов: 953\n",
      "Предсказано не-нулей: 19613 / 19613\n",
      "Среднее количество хабов на статью: 3.90\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T05:56:29.954858800Z",
     "start_time": "2025-11-20T05:48:49.581010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import yake\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Загружаем processed_habr.pkl...\")\n",
    "if not os.path.exists(\"data/processed_habr.pkl\"):\n",
    "    raise FileNotFoundError(\"Файл processed_habr.pkl не найден!\")\n",
    "\n",
    "df = pd.read_pickle(\"data/processed_habr.pkl\")\n",
    "print(f\"Исходный размер датасета: {len(df)}\")\n",
    "\n",
    "N_SAMPLES = 20000\n",
    "if len(df) > N_SAMPLES:\n",
    "    df_sample = df.head(N_SAMPLES).copy()\n",
    "    print(f\"Обрабатываем выборку: {len(df_sample)} записей (первые {N_SAMPLES})\")\n",
    "else:\n",
    "    df_sample = df.copy()\n",
    "    print(f\"Датасет меньше {N_SAMPLES}, обрабатываем все {len(df_sample)} записей\")\n",
    "\n",
    "\n",
    "kw_extractor = yake.KeywordExtractor(\n",
    "    lan=\"ru\",\n",
    "    n=3,\n",
    "    top=15,\n",
    "    dedupLim=0.7,\n",
    "    features=None\n",
    ")\n",
    "\n",
    "russian_stopwords = {\n",
    "    'и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со',\n",
    "    'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'у', 'же',\n",
    "    'бы', 'для', 'по', 'о', 'от', 'из', 'к', 'об', 'при', 'над', 'под'\n",
    "}\n",
    "\n",
    "def extractKeys(text):\n",
    "    \"\"\"Извлекает ключевые фразы с помощью YAKE.\"\"\"\n",
    "    if not isinstance(text, str) or len(text.strip()) < 10:\n",
    "        return []\n",
    "    try:\n",
    "        keywords = kw_extractor.extract_keywords(text)\n",
    "        result = []\n",
    "        for phrase, score in keywords:\n",
    "            words = phrase.lower().split()\n",
    "            if any(word not in russian_stopwords for word in words):\n",
    "                result.append(phrase.strip())\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при обработке текста: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"Извлекаем ключевые слова из cleaned_text с помощью YAKE...\")\n",
    "df_sample['text_main'] = pd.Series(tqdm(\n",
    "    (extractKeys(text) for text in df_sample['cleaned_text']),\n",
    "    total=len(df_sample),\n",
    "    desc=\"YAKE: извлечение ключевых фраз\",\n",
    "    unit=\"текст\"\n",
    "))\n",
    "\n",
    "print(\"Сохраняем промежуточный результат после YAKE...\")\n",
    "df_sample.to_pickle(\"data/sample_20k_yake_extracted.pkl\")\n",
    "print(\"Сохранено: data/sample_20k_yake_extracted.pkl\")\n",
    "\n",
    "print(\"Очищаем извлечённые ключевые фразы...\")\n",
    "\n",
    "def clean_keywords(keywords):\n",
    "    if not isinstance(keywords, list):\n",
    "        return []\n",
    "    \n",
    "    if not keywords:\n",
    "        return []\n",
    "    \n",
    "    cleaned = []\n",
    "    for phrase in keywords:\n",
    "        if not isinstance(phrase, str):\n",
    "            phrase = str(phrase)\n",
    "        \n",
    "        phrase = re.sub(r'[^а-яА-Яёёa-zA-Z0-9\\s]', ' ', phrase)\n",
    "        phrase = re.sub(r'\\s+', ' ', phrase).strip()\n",
    "        if phrase:\n",
    "            cleaned.append(phrase)\n",
    "    return cleaned\n",
    "\n",
    "df_sample['text_main'] = df_sample['text_main'].apply(clean_keywords)\n",
    "\n",
    "print(\"Дополнительная очистка: удаление дубликатов и нормализация...\")\n",
    "def final_clean(keywords):\n",
    "    if not keywords:\n",
    "        return []\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for k in keywords:\n",
    "        if k not in seen:\n",
    "            seen.add(k)\n",
    "            result.append(k)\n",
    "    return result\n",
    "\n",
    "df_sample['text_main'] = df_sample['text_main'].apply(final_clean)\n",
    "\n",
    "print(\"Сохраняем итоговые данные...\")\n",
    "df_sample.to_pickle(\"data/sample_20k_with_keywords.pkl\")\n",
    "df_sample.to_csv(\"data/sample_20k_keywords.csv\", index=False, encoding='utf-8')\n",
    "\n",
    "\n",
    "print(\"\\nГотово!\")\n",
    "print(f\"Обработано: {len(df_sample)} текстов\")\n",
    "print(f\"Итоговый файл: sample_20k_with_keywords.pkl\")\n",
    "print(f\"CSV-версия: sample_20k_keywords.csv\")\n",
    "\n",
    "print(f\"\\nПример первых 15 значений 'text_main':\")\n",
    "for i in range(15):\n",
    "    if i < len(df_sample):\n",
    "        print(f\"{i}: {df_sample['text_main'].iloc[i]}\")\n"
   ],
   "id": "8e9f6fa7cbac54b0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружаем processed_habr.pkl...\n",
      "Исходный размер датасета: 98064\n",
      "Обрабатываем выборку: 20000 записей (первые 20000)\n",
      "Извлекаем ключевые слова из cleaned_text с помощью YAKE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YAKE: извлечение ключевых фраз: 100%|██████████| 20000/20000 [56:15<00:00,  5.92текст/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сохраняем промежуточный результат после YAKE...\n",
      "Сохранено: sample_20k_yake_extracted.pkl\n",
      "Очищаем извлечённые ключевые фразы...\n",
      "Дополнительная очистка: удаление дубликатов и нормализация...\n",
      "Сохраняем итоговые данные...\n",
      "\n",
      "Готово!\n",
      "Обработано: 20000 текстов\n",
      "Итоговый файл: sample_20k_with_keywords.pkl\n",
      "CSV-версия: sample_20k_keywords.csv\n",
      "\n",
      "Пример первых 15 значений 'text_main':\n",
      "0: ['компанию либо самый', 'либо самый умный', 'самый умный либо', 'умный либо самый', 'либо самый глупый', 'википедия небольшая компания', 'википедия небогатая компания', 'статей включая разделы', 'например шведская википедия', 'языков бла бла', 'исчезающе малы например', 'малы например родная', 'целью набрать статей', 'подавляющая часть статей', 'миллиардах просмотров статей']\n",
      "1: ['apple newton messagepad', 'дополненной реальности google', 'продукт компании google', 'очки google glass', 'реальности google glassgoogle', 'смарт очки google', 'университета apple newton', 'glass продукт компании', 'производства dvd могла', 'атмосферу виртуальной реальности', 'виртуальной реальности инженер', 'сфере виртуальной реальности', 'виртуальной реальности решила', 'шлеме виртуальной реальности', 'очки дополненной реальности']\n",
      "2: ['читать слушать ответы', 'подкаста читать слушать', 'акустики читать слушать', 'звука читать слушать', 'читать слушать обсуждаем', 'читать слушать простыми', 'читать слушать говорим', 'читать слушать акустика', 'кабели читать слушать', 'вопросы читать слушать', 'читать слушать', 'слушателей читать слушать', 'читать слушать мнение', 'читать слушать музыкальный', 'ситуация читать слушать']\n",
      "3: ['регулярный прием парацетамола', 'интервью профессора ирен', 'полезно регулярный прием', 'профессора ирен брейтуэйт', 'ирен брейтуэйт заместителя', 'зеландия ирен брейтуэйт', 'инфекции является вакцинация', 'прием жаропонижающих средств', 'гриппозной инфекции является', 'брейтуэйт заместителя директора', 'новая зеландия ирен', 'взрослых людей инфицированных', 'эффект действия парацетамола', 'действия парацетамола грамм', 'лечению внебольничной инфекции']\n",
      "4: ['случаев рака связан', 'причиной случаев рака', 'избежать случаев рака', 'предотвратить случаев рака', 'семи случаев рака', 'рака рака лёгких', 'риск рака груди', 'рака колоректального рака', 'колоректального рака рака', 'случаев рака ануса', 'случаев рака полового', 'случаев вагинального рака', 'случаев меланомы рака', 'шанс возникновения рака', 'альберте вызывает случаев']\n",
      "5: ['пропускная способность канала', 'недоволен пропускная способность', 'средняя пропускная способность', 'пропускная способность сетевого', 'способность сетевого порта', 'zero фото raspberry', 'порта малинки мбит', 'словам средняя пропускная', 'способность канала составляетоколо', 'сообщение следующее hey', 'некоторых регионах ситуация', 'столице сша вашингтоне', 'житель вашингтона известен', 'решил наладить автоматическую', 'наладить автоматическую отправку']\n",
      "6: ['параболических зеркал расположены', 'стоимость млн евро', 'noor iii мвт', 'метровых параболических зеркал', 'iii мвт млн', 'мвт млн евро', 'характеристики проекта noor', 'больших метровых параболических', 'csp станции noor', 'энергии строительство noor', 'мвт стоимость млн', 'пустыне сахара состояласьцеремония', 'сахара состояласьцеремония торжественного', 'состояласьцеремония торжественного открытиякрупнейшей', 'concentrating solar power']\n",
      "7: ['геномы клопов найденных', 'решение расшифровка генома', 'расшифровка генома клопа', 'расшифровка генома такого', 'пришлось сравнивать геномы', 'результаты удивляют геномы', 'например отличаются геномы', 'сравнивать геномы клопов', 'геномы клопов начиная', 'удивляют геномы клопов', 'геномы клопов различаются', 'отличаются геномы клопов', 'фото brian kersey', 'оптимальный метод борьбы', 'генома такого организма']\n",
      "8: ['usb дисководе usb', 'умеет использовать usb', 'dos игр накопитель', 'usb клавиатура итак', 'дисководе usb клавиатура', 'работать usb клавиатура', 'usb флоппи дисководе', 'старых dos игр', 'известные игры например', 'dos успешно загрузился', 'целью нашего путешествия', 'раритетный компьютер поэтому', 'флешки достаточно поставить', 'достаточно слабый компьютер', 'самые известные игры']\n",
      "9: ['облаке газа пускай', 'облаке должны находиться', 'откуда выброшено облако', 'галактики откуда выброшено', 'вернется облако смит', 'облако является примером', 'диске галактики откуда', 'образоваться примерно млн', 'примерно млн новых', 'названное облаком смит', 'первооткрывательницы гейлы смит', 'гейлы смит обнаружившей', 'смит обнаружившей объект', 'смит засекла радиоволны', 'глаза человека облако']\n",
      "10: ['ранней стадии сигнальная', 'пяти разных видов', 'пяти разных видах', 'разных видов рака', 'стадии сигнальная геномная', 'разных видах рака', 'выявили геномную сигнатуру', 'нашли метилированную сигнатуру', 'выявили уникальную сигнатуру', 'большого количества видов', 'количества видов рака', 'видов рака восхищение', 'крови диагностировать сразу', 'анализом крови диагностировать', 'биолог лаура ельнитски']\n",
      "11: ['финансовым департаментом alphabet', 'млрд относительно млрд', 'дохода alphabet млрд', 'alphabet млрд обеспечили', 'компания google alphabet', 'alphabet результаты компании', 'google alphabet предоставила', 'капитализации компаний apple', 'департаментом alphabet результаты', 'финансовые результаты google', 'подразделением alphabet google', 'сервиса google alphabet', 'млрд соответственно выручка', 'alphabet предоставила квартальный', 'google обгоняла apple']\n",
      "12: ['камерой mpза бесплатно', 'geektimes приятного просмотра', 'mpза бесплатно доставляется', 'россию характеристики аккумулятор', 'cmвес грматериал пластикколичество', 'грматериал пластикколичество каналов', 'пластикколичество каналов частота', 'ghzдальность связи метровкамера', 'комплектации комплектация поставки', 'usb зарядка кабель', 'любые препятствия соединяя', 'выбора сложной техникиdronk', 'стоимости покупки приобретая', 'покупки приобретая товар', 'ссылкам подробнее dronk']\n",
      "13: ['качестве отдельного проекта', 'качестве отдельного приложения', 'функция виртуальной реальности', 'такойшлем виртуальной реальности', 'совсемgoogle cardboard новое', 'версия google cardboard', 'касается google cardboard', 'виртуальную реальность google', 'google cardboard устройство', 'корпорация планирует выпустить', 'планирует выпустить девайс', 'выпустить очередную модель', 'линзы плюс используется', 'очередную модель видеоочков', 'пишетbusinessinsider корпорация планирует']\n",
      "14: ['самолётам илон маск', 'источник питания оказался', 'вспомогательный источник питания', 'полностью электрический самолёт', 'маск хочет создать', 'питания оказался значительно', 'литий ионные аккумуляторы', 'хочет делать электрические', 'упоминает электрические самолёты', 'электрические самолёты вспоминаютжурналисты', 'самолётов пытались использовать', 'делать электрические двигатели', 'летом реально построить', 'проект реально начнут', 'реально начнут вообще']\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T05:56:29.954858800Z",
     "start_time": "2025-11-20T08:17:46.959401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import jaccard_score, hamming_loss, f1_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from scipy.sparse import hstack  \n",
    "import time\n",
    "\n",
    "df = pd.read_pickle(\"data/sample_20k_with_keywords.pkl\")\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['hubs'])\n",
    "valid_hub_mask = y.sum(axis=0) >= 2 \n",
    "y_filtered = y[:, valid_hub_mask]\n",
    "\n",
    "filtered_hubs = [\n",
    "    [hub for hub in hubs if hub in mlb.classes_[valid_hub_mask]]\n",
    "    for hubs in df['hubs']\n",
    "]\n",
    "mlb_filtered = MultiLabelBinarizer()\n",
    "y = mlb_filtered.fit_transform(filtered_hubs)\n",
    "\n",
    "non_empty = y.sum(axis=1) > 0\n",
    "df = df[non_empty].copy()\n",
    "y = y[non_empty]\n",
    "\n",
    "print(f\"Статей после фильтрации: {len(df)}, хабов: {y.shape[1]}\")\n",
    "\n",
    "print(\"Векторизация...\")\n",
    "start_time = time.time()\n",
    "\n",
    "df['text_main_str'] = df['text_main'].apply(lambda x: ' '.join(x) if x else '')\n",
    "df['cleaned_keywords_str'] = df['cleaned_keywords'].fillna('').astype(str)\n",
    "df['full_text_with_yake'] = (\n",
    "    df['cleaned_title'].fillna('') + ' ' +\n",
    "    df['cleaned_keywords_str'] + ' ' +\n",
    "    df['text_main_str']\n",
    ")\n",
    "\n",
    "mask = df['full_text_with_yake'].str.len() > 50\n",
    "df = df[mask]\n",
    "y = y[mask.values] \n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=20000,\n",
    "    ngram_range=(1, 4),\n",
    "    min_df=3,\n",
    "    max_df=0.75,\n",
    "    stop_words=[\n",
    "        'и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со',\n",
    "        'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да'\n",
    "    ],\n",
    "    sublinear_tf=True,\n",
    "    lowercase=True,\n",
    "    smooth_idf=True,\n",
    "    norm='l2'\n",
    ")\n",
    "X_text = vectorizer.fit_transform(df['full_text_with_yake'])\n",
    "\n",
    "username_vectorizer = TfidfVectorizer(\n",
    "    analyzer='char_wb',      \n",
    "    ngram_range=(2, 4),    \n",
    "    min_df=1,             \n",
    "    max_features=500,     \n",
    "    sublinear_tf=True,\n",
    "    lowercase=True\n",
    ")\n",
    "X_username = username_vectorizer.fit_transform(df['username'].fillna(''))\n",
    "\n",
    "X = hstack([X_text, X_username])  \n",
    "\n",
    "print(f\"!Векторизация за {time.time() - start_time:.2f} сек, X: {X.shape}, y: {y.shape}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Обучение модели...\")\n",
    "modelNew = MultiOutputClassifier(\n",
    "    LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        class_weight='balanced',\n",
    "        C=0.5,\n",
    "        solver='liblinear',\n",
    "        random_state=42\n",
    "    ),\n",
    "    n_jobs=-1\n",
    ")\n",
    "modelNew.fit(X_train, y_train)\n",
    "\n",
    "y_pred = modelNew.predict(X_test)\n",
    "jaccard = jaccard_score(y_test, y_pred, average='samples')\n",
    "hamming = hamming_loss(y_test, y_pred)\n",
    "\n",
    "print(f\"Результаты:\")\n",
    "print(f\"Jaccard Score: {jaccard:.4f}\")\n",
    "print(f\"Hamming Loss:  {hamming:.4f}\")\n",
    "print(f\"Классов: {y.shape[1]}, статей: {len(df)}\")\n"
   ],
   "id": "77fe4065e637999f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статей после фильтрации: 20000, хабов: 698\n",
      "Векторизация...\n",
      "!Векторизация за 9.94 сек, X: (19764, 20500), y: (19764, 698)\n",
      "Обучение модели...\n",
      "\n",
      "Результаты:\n",
      "Jaccard Score: 0.3788\n",
      "Hamming Loss:  0.0073\n",
      "Классов: 698, статей: 19764\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T05:56:29.955850Z",
     "start_time": "2025-11-20T14:44:54.386654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import yake\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Загружаем processed_habr.pkl...\")\n",
    "if not os.path.exists(\"data/processed_habr.pkl\"):\n",
    "    raise FileNotFoundError(\"Файл processed_habr.pkl не найден!\")\n",
    "\n",
    "df = pd.read_pickle(\"data/processed_habr.pkl\")\n",
    "print(f\"Исходный размер датасета: {len(df)}\")\n",
    "\n",
    "df_sample = df.copy()\n",
    "\n",
    "kw_extractor = yake.KeywordExtractor(\n",
    "    lan=\"ru\",\n",
    "    n=3,\n",
    "    top=15,\n",
    "    dedupLim=0.7,\n",
    "    features=None\n",
    ")\n",
    "\n",
    "russian_stopwords = {\n",
    "    'и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со',\n",
    "    'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'у', 'же',\n",
    "    'бы', 'для', 'по', 'о', 'от', 'из', 'к', 'об', 'при', 'над', 'под'\n",
    "}\n",
    "\n",
    "def extractKeys(text):\n",
    "    if not isinstance(text, str) or len(text.strip()) < 10:\n",
    "        return []\n",
    "    try:\n",
    "        keywords = kw_extractor.extract_keywords(text)\n",
    "        result = []\n",
    "        for phrase, score in keywords:\n",
    "            words = phrase.lower().split()\n",
    "            if any(word not in russian_stopwords for word in words):\n",
    "                result.append(phrase.strip())\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при обработке текста: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"Извлекаем ключевые слова из cleaned_text с помощью YAKE...\")\n",
    "df_sample['text_main'] = pd.Series(tqdm(\n",
    "    (extractKeys(text) for text in df_sample['cleaned_text']),\n",
    "    total=len(df_sample),\n",
    "    desc=\"YAKE: извлечение ключевых фраз\",\n",
    "    unit=\"текст\"\n",
    "))\n",
    "\n",
    "print(\"Сохраняем промежуточный результат после YAKE...\")\n",
    "df_sample.to_pickle(\"data_with_main_info_extract.pkl\")\n",
    "print(\"Сохранено: data_with_main_info_extract.pkl\")\n",
    "\n",
    "print(\"Очищаем извлечённые ключевые фразы...\")\n",
    "\n",
    "def clean_keywords(keywords):\n",
    "    if not isinstance(keywords, list):\n",
    "        return []\n",
    "    \n",
    "    if not keywords:\n",
    "        return []\n",
    "    \n",
    "    cleaned = []\n",
    "    for phrase in keywords:\n",
    "        if not isinstance(phrase, str):\n",
    "            phrase = str(phrase)\n",
    "        \n",
    "        phrase = re.sub(r'[^а-яА-Яёёa-zA-Z0-9\\s]', ' ', phrase)\n",
    "        phrase = re.sub(r'\\s+', ' ', phrase).strip()\n",
    "        if phrase:\n",
    "            cleaned.append(phrase)\n",
    "    return cleaned\n",
    "\n",
    "df_sample['text_main'] = df_sample['text_main'].apply(clean_keywords)\n",
    "\n",
    "print(\"Дополнительная очистка: удаление дубликатов и нормализация...\")\n",
    "def final_clean(keywords):\n",
    "    if not keywords:\n",
    "        return []\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for k in keywords:\n",
    "        if k not in seen:\n",
    "            seen.add(k)\n",
    "            result.append(k)\n",
    "    return result\n",
    "\n",
    "df_sample['text_main'] = df_sample['text_main'].apply(final_clean)\n",
    "\n",
    "print(\"Сохраняем итоговые данные...\")\n",
    "df_sample.to_pickle(\"data_with_main_info.pkl\")\n",
    "df_sample.to_csv(\"data_with_main_info.csv\", index=False, encoding='utf-8')\n",
    "\n",
    "\n",
    "print(\"\\nГотово!\")\n",
    "print(f\"Обработано: {len(df_sample)} текстов\")\n",
    "print(f\"Итоговый файл: data_with_main_info.pkl\")\n",
    "print(f\"CSV-версия: data_with_main_info.csv\")\n",
    "\n",
    "print(f\"\\nПример первых 15 значений 'text_main':\")\n",
    "for i in range(15):\n",
    "    if i < len(df_sample):\n",
    "        print(f\"{i}: {df_sample['text_main'].iloc[i]}\")\n"
   ],
   "id": "170df86afa91fa36",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружаем processed_habr.pkl...\n",
      "Исходный размер датасета: 98064\n",
      "Извлекаем ключевые слова из cleaned_text с помощью YAKE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YAKE: извлечение ключевых фраз: 100%|██████████| 98064/98064 [4:36:28<00:00,  5.91текст/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сохраняем промежуточный результат после YAKE...\n",
      "Сохранено: data_with_main_info_extract.pkl\n",
      "Очищаем извлечённые ключевые фразы...\n",
      "Дополнительная очистка: удаление дубликатов и нормализация...\n",
      "Сохраняем итоговые данные...\n",
      "\n",
      "Готово!\n",
      "Обработано: 98064 текстов\n",
      "Итоговый файл: data_with_main_info.pkl\n",
      "CSV-версия: data_with_main_info.csv\n",
      "\n",
      "Пример первых 15 значений 'text_main':\n",
      "0: ['компанию либо самый', 'либо самый умный', 'самый умный либо', 'умный либо самый', 'либо самый глупый', 'википедия небольшая компания', 'википедия небогатая компания', 'статей включая разделы', 'например шведская википедия', 'языков бла бла', 'исчезающе малы например', 'малы например родная', 'целью набрать статей', 'подавляющая часть статей', 'миллиардах просмотров статей']\n",
      "1: ['apple newton messagepad', 'дополненной реальности google', 'продукт компании google', 'очки google glass', 'реальности google glassgoogle', 'смарт очки google', 'университета apple newton', 'glass продукт компании', 'производства dvd могла', 'атмосферу виртуальной реальности', 'виртуальной реальности инженер', 'сфере виртуальной реальности', 'виртуальной реальности решила', 'шлеме виртуальной реальности', 'очки дополненной реальности']\n",
      "2: ['читать слушать ответы', 'подкаста читать слушать', 'акустики читать слушать', 'звука читать слушать', 'читать слушать обсуждаем', 'читать слушать простыми', 'читать слушать говорим', 'читать слушать акустика', 'кабели читать слушать', 'вопросы читать слушать', 'читать слушать', 'слушателей читать слушать', 'читать слушать мнение', 'читать слушать музыкальный', 'ситуация читать слушать']\n",
      "3: ['регулярный прием парацетамола', 'интервью профессора ирен', 'полезно регулярный прием', 'профессора ирен брейтуэйт', 'ирен брейтуэйт заместителя', 'зеландия ирен брейтуэйт', 'инфекции является вакцинация', 'прием жаропонижающих средств', 'гриппозной инфекции является', 'брейтуэйт заместителя директора', 'новая зеландия ирен', 'взрослых людей инфицированных', 'эффект действия парацетамола', 'действия парацетамола грамм', 'лечению внебольничной инфекции']\n",
      "4: ['случаев рака связан', 'причиной случаев рака', 'избежать случаев рака', 'предотвратить случаев рака', 'семи случаев рака', 'рака рака лёгких', 'риск рака груди', 'рака колоректального рака', 'колоректального рака рака', 'случаев рака ануса', 'случаев рака полового', 'случаев вагинального рака', 'случаев меланомы рака', 'шанс возникновения рака', 'альберте вызывает случаев']\n",
      "5: ['пропускная способность канала', 'недоволен пропускная способность', 'средняя пропускная способность', 'пропускная способность сетевого', 'способность сетевого порта', 'zero фото raspberry', 'порта малинки мбит', 'словам средняя пропускная', 'способность канала составляетоколо', 'сообщение следующее hey', 'некоторых регионах ситуация', 'столице сша вашингтоне', 'житель вашингтона известен', 'решил наладить автоматическую', 'наладить автоматическую отправку']\n",
      "6: ['параболических зеркал расположены', 'стоимость млн евро', 'noor iii мвт', 'метровых параболических зеркал', 'iii мвт млн', 'мвт млн евро', 'характеристики проекта noor', 'больших метровых параболических', 'csp станции noor', 'энергии строительство noor', 'мвт стоимость млн', 'пустыне сахара состояласьцеремония', 'сахара состояласьцеремония торжественного', 'состояласьцеремония торжественного открытиякрупнейшей', 'concentrating solar power']\n",
      "7: ['геномы клопов найденных', 'решение расшифровка генома', 'расшифровка генома клопа', 'расшифровка генома такого', 'пришлось сравнивать геномы', 'результаты удивляют геномы', 'например отличаются геномы', 'сравнивать геномы клопов', 'геномы клопов начиная', 'удивляют геномы клопов', 'геномы клопов различаются', 'отличаются геномы клопов', 'фото brian kersey', 'оптимальный метод борьбы', 'генома такого организма']\n",
      "8: ['usb дисководе usb', 'умеет использовать usb', 'dos игр накопитель', 'usb клавиатура итак', 'дисководе usb клавиатура', 'работать usb клавиатура', 'usb флоппи дисководе', 'старых dos игр', 'известные игры например', 'dos успешно загрузился', 'целью нашего путешествия', 'раритетный компьютер поэтому', 'флешки достаточно поставить', 'достаточно слабый компьютер', 'самые известные игры']\n",
      "9: ['облаке газа пускай', 'облаке должны находиться', 'откуда выброшено облако', 'галактики откуда выброшено', 'вернется облако смит', 'облако является примером', 'диске галактики откуда', 'образоваться примерно млн', 'примерно млн новых', 'названное облаком смит', 'первооткрывательницы гейлы смит', 'гейлы смит обнаружившей', 'смит обнаружившей объект', 'смит засекла радиоволны', 'глаза человека облако']\n",
      "10: ['ранней стадии сигнальная', 'пяти разных видов', 'пяти разных видах', 'разных видов рака', 'стадии сигнальная геномная', 'разных видах рака', 'выявили геномную сигнатуру', 'нашли метилированную сигнатуру', 'выявили уникальную сигнатуру', 'большого количества видов', 'количества видов рака', 'видов рака восхищение', 'крови диагностировать сразу', 'анализом крови диагностировать', 'биолог лаура ельнитски']\n",
      "11: ['финансовым департаментом alphabet', 'млрд относительно млрд', 'дохода alphabet млрд', 'alphabet млрд обеспечили', 'компания google alphabet', 'alphabet результаты компании', 'google alphabet предоставила', 'капитализации компаний apple', 'департаментом alphabet результаты', 'финансовые результаты google', 'подразделением alphabet google', 'сервиса google alphabet', 'млрд соответственно выручка', 'alphabet предоставила квартальный', 'google обгоняла apple']\n",
      "12: ['камерой mpза бесплатно', 'geektimes приятного просмотра', 'mpза бесплатно доставляется', 'россию характеристики аккумулятор', 'cmвес грматериал пластикколичество', 'грматериал пластикколичество каналов', 'пластикколичество каналов частота', 'ghzдальность связи метровкамера', 'комплектации комплектация поставки', 'usb зарядка кабель', 'любые препятствия соединяя', 'выбора сложной техникиdronk', 'стоимости покупки приобретая', 'покупки приобретая товар', 'ссылкам подробнее dronk']\n",
      "13: ['качестве отдельного проекта', 'качестве отдельного приложения', 'функция виртуальной реальности', 'такойшлем виртуальной реальности', 'совсемgoogle cardboard новое', 'версия google cardboard', 'касается google cardboard', 'виртуальную реальность google', 'google cardboard устройство', 'корпорация планирует выпустить', 'планирует выпустить девайс', 'выпустить очередную модель', 'линзы плюс используется', 'очередную модель видеоочков', 'пишетbusinessinsider корпорация планирует']\n",
      "14: ['самолётам илон маск', 'источник питания оказался', 'вспомогательный источник питания', 'полностью электрический самолёт', 'маск хочет создать', 'питания оказался значительно', 'литий ионные аккумуляторы', 'хочет делать электрические', 'упоминает электрические самолёты', 'электрические самолёты вспоминаютжурналисты', 'самолётов пытались использовать', 'делать электрические двигатели', 'летом реально построить', 'проект реально начнут', 'реально начнут вообще']\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T05:56:29.955850Z",
     "start_time": "2025-11-20T20:49:30.777829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import jaccard_score, hamming_loss, f1_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, OneHotEncoder\n",
    "from scipy.sparse import hstack \n",
    "import time\n",
    "import numpy as np\n",
    "import joblib  \n",
    "\n",
    "df = pd.read_pickle(\"data/data_with_main_info.pkl\")\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['hubs'])\n",
    "valid_hub_mask = y.sum(axis=0) >= 2  \n",
    "y_filtered = y[:, valid_hub_mask]\n",
    "\n",
    "filtered_hubs = [\n",
    "    [hub for hub in hubs if hub in mlb.classes_[valid_hub_mask]]\n",
    "    for hubs in df['hubs']\n",
    "]\n",
    "mlb_filtered = MultiLabelBinarizer()\n",
    "y = mlb_filtered.fit_transform(filtered_hubs)\n",
    "\n",
    "non_empty = y.sum(axis=1) > 0\n",
    "df = df[non_empty].copy()\n",
    "y = y[non_empty]\n",
    "\n",
    "print(f\"Статей после фильтрации: {len(df)}, хабов: {y.shape[1]}\")\n",
    "\n",
    "print(\"Векторизация...\")\n",
    "start_time = time.time()\n",
    "\n",
    "df['text_main_str'] = df['text_main'].apply(lambda x: ' '.join(x) if x else '')\n",
    "df['cleaned_keywords_str'] = df['cleaned_keywords'].fillna('').astype(str)\n",
    "df['full_text_with_yake'] = (\n",
    "    df['cleaned_title'].fillna('') + ' ' +\n",
    "    df['cleaned_keywords_str'] + ' ' +\n",
    "    df['text_main_str']\n",
    ")\n",
    "\n",
    "mask = df['full_text_with_yake'].str.len() > 50\n",
    "df = df[mask]\n",
    "y = y[mask.values] \n",
    "\n",
    "df['datetime'] = pd.to_datetime(df['time'])\n",
    "df['year'] = df['datetime'].dt.year\n",
    "df['month'] = df['datetime'].dt.month\n",
    "df['dayofweek'] = df['datetime'].dt.dayofweek\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "\n",
    "time_features = ['year', 'month', 'dayofweek', 'hour']\n",
    "time_encoder = OneHotEncoder(sparse_output=True, drop='first')\n",
    "X_time = time_encoder.fit_transform(df[time_features])\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=40000,           \n",
    "    ngram_range=(1, 3),           \n",
    "    min_df=2,                     \n",
    "    max_df=0.8,                \n",
    "    stop_words=[\n",
    "        'и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со',\n",
    "        'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да'\n",
    "    ],\n",
    "    sublinear_tf=True,\n",
    "    lowercase=True,\n",
    "    smooth_idf=True,\n",
    "    norm='l2',\n",
    "    use_idf=True,\n",
    "    binary=False\n",
    ")\n",
    "X_text = vectorizer.fit_transform(df['full_text_with_yake'])\n",
    "\n",
    "username_vectorizer = TfidfVectorizer(\n",
    "    analyzer='char_wb',\n",
    "    ngram_range=(2, 4),\n",
    "    min_df=1,\n",
    "    max_features=1500,\n",
    "    sublinear_tf=True,\n",
    "    lowercase=True\n",
    ")\n",
    "X_username = username_vectorizer.fit_transform(df['username'].fillna(''))\n",
    "\n",
    "X = hstack([X_text, X_username, X_time])\n",
    "\n",
    "print(f\"!Векторизация за {time.time() - start_time:.2f} сек, X: {X.shape}, y: {y.shape}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Обучение модели...\")\n",
    "modelFull = MultiOutputClassifier(\n",
    "    LogisticRegression(\n",
    "        max_iter=2000,\n",
    "        class_weight='balanced',\n",
    "        C=0.5,\n",
    "        solver='liblinear',\n",
    "        random_state=42,\n",
    "        tol=1e-4,\n",
    "        fit_intercept=True\n",
    "    ),\n",
    "    n_jobs=-1\n",
    ")\n",
    "modelFull.fit(X_train, y_train)\n",
    "\n",
    "y_pred = modelFull.predict(X_test)\n",
    "jaccard = jaccard_score(y_test, y_pred, average='samples')\n",
    "hamming = hamming_loss(y_test, y_pred)\n",
    "\n",
    "print(f\"Результаты:\")\n",
    "print(f\"Jaccard Score: {jaccard:.4f}\")\n",
    "print(f\"Hamming Loss:  {hamming:.4f}\")\n",
    "print(f\"Классов: {y.shape[1]}, статей: {len(df)}\")\n",
    "\n",
    "joblib.dump(vectorizer, 'vectorizer.pkl')\n",
    "joblib.dump(username_vectorizer, 'username_vectorizer.pkl')\n",
    "joblib.dump(time_encoder, 'time_encoder.pkl')\n",
    "joblib.dump(mlb_filtered, 'mlb_filtered.pkl')\n",
    "joblib.dump(modelFull, 'helpers/modelFull.pkl')  \n"
   ],
   "id": "bbcb15e359845800",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статей после фильтрации: 98064, хабов: 953\n",
      "Векторизация...\n",
      "!Векторизация за 43.58 сек, X: (97731, 41549), y: (97731, 953)\n",
      "Обучение модели...\n",
      "\n",
      "Результаты:\n",
      "Jaccard Score: 0.3700\n",
      "Hamming Loss:  0.0060\n",
      "Классов: 953, статей: 97731\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['modelFull.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T19:22:15.037250Z",
     "start_time": "2025-12-02T18:44:34.673561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_predict\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import jaccard_score, hamming_loss, f1_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, OneHotEncoder, StandardScaler\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import time\n",
    "import numpy as np\n",
    "import joblib\n",
    "from collections import Counter\n",
    "\n",
    "df = pd.read_pickle(\"data/data_with_main_info.pkl\")\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['hubs'])\n",
    "\n",
    "min_samples_per_class = max(3, int(len(df) * 0.001))  \n",
    "valid_hub_mask = y.sum(axis=0) >= min_samples_per_class\n",
    "y_filtered = y[:, valid_hub_mask]\n",
    "\n",
    "filtered_hubs = [\n",
    "    [hub for hub in hubs if hub in mlb.classes_[valid_hub_mask]]\n",
    "    for hubs in df['hubs']\n",
    "]\n",
    "mlb_filtered = MultiLabelBinarizer()\n",
    "y = mlb_filtered.fit_transform(filtered_hubs)\n",
    "\n",
    "non_empty = y.sum(axis=1) > 0\n",
    "df = df[non_empty].copy()\n",
    "y = y[non_empty]\n",
    "\n",
    "print(f\"Статей после фильтрации: {len(df)}, хабов: {y.shape[1]}\")\n",
    "print(f\"Распределение хабов: {dict(Counter(y.sum(axis=1)).most_common(10))}\")\n",
    "\n",
    "print(\"\\nВекторизация...\")\n",
    "start_time = time.time()\n",
    "\n",
    "df['text_main_str'] = df['text_main'].apply(lambda x: ' '.join(x) if x else '')\n",
    "df['cleaned_keywords_str'] = df['cleaned_keywords'].fillna('').astype(str)\n",
    "\n",
    "df['full_text_weighted'] = (\n",
    "    df['cleaned_title'].fillna('') + ' ' +\n",
    "    df['cleaned_title'].fillna('') + ' ' +\n",
    "    df['cleaned_keywords_str'] + ' ' +\n",
    "    df['text_main_str']\n",
    ")\n",
    "\n",
    "min_text_length = 50\n",
    "mask = df['full_text_weighted'].str.len() > min_text_length\n",
    "df = df[mask]\n",
    "y = y[mask.values]\n",
    "\n",
    "df['datetime'] = pd.to_datetime(df['time'])\n",
    "df['year'] = df['datetime'].dt.year\n",
    "df['month'] = df['datetime'].dt.month\n",
    "df['dayofweek'] = df['datetime'].dt.dayofweek\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['is_weekend'] = df['dayofweek'].isin([5, 6]).astype(int)\n",
    "df['quarter'] = df['datetime'].dt.quarter\n",
    "\n",
    "time_features = ['year', 'month', 'dayofweek', 'hour', 'is_weekend', 'quarter']\n",
    "time_encoder = OneHotEncoder(sparse_output=True, drop='first')\n",
    "X_time = time_encoder.fit_transform(df[time_features])\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=30000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.8,\n",
    "    stop_words=[\n",
    "        'и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со',\n",
    "        'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да',\n",
    "        'ты', 'по', 'но', 'за', 'из', 'это', 'или', 'у', 'же', 'бы',\n",
    "        'вот', 'от', 'меня', 'ему', 'нет', 'о', 'еще', 'когда',\n",
    "        'даже', 'ну', 'ли', 'если', 'был', 'до', 'ни', 'быть',\n",
    "        'при', 'также', 'к', 'по', 'на', 'этот', 'что', 'который'\n",
    "    ],\n",
    "    sublinear_tf=True,\n",
    "    lowercase=True,\n",
    "    smooth_idf=True,\n",
    "    norm='l2',\n",
    "    use_idf=True,\n",
    "    binary=False,\n",
    "    analyzer='word'\n",
    ")\n",
    "\n",
    "X_text = vectorizer.fit_transform(df['full_text_weighted'])\n",
    "\n",
    "username_vectorizer = TfidfVectorizer(\n",
    "    analyzer='char',\n",
    "    ngram_range=(3, 4),\n",
    "    min_df=2,\n",
    "    max_features=1500,\n",
    "    sublinear_tf=True,\n",
    "    lowercase=True,\n",
    "    binary=True\n",
    ")\n",
    "X_username = username_vectorizer.fit_transform(df['username'].fillna(''))\n",
    "\n",
    "df['text_length'] = df['full_text_weighted'].str.len()\n",
    "df['word_count'] = df['full_text_weighted'].str.split().str.len()\n",
    "\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "X_numeric = csr_matrix(scaler.fit_transform(df[['text_length', 'word_count']]))\n",
    "\n",
    "X = hstack([X_text, X_username, X_time, X_numeric])\n",
    "print(f\"Векторизация за {time.time() - start_time:.2f} сек, X: {X.shape}, y: {y.shape}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nРазмер train: {X_train.shape}, test: {X_test.shape}\")\n",
    "\n",
    "print(\"\\nОбучение модели...\")\n",
    "model = MultiOutputClassifier(\n",
    "    LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        class_weight='balanced',\n",
    "        C=0.8,\n",
    "        solver='liblinear',\n",
    "        penalty='l2',\n",
    "        random_state=42,\n",
    "        tol=1e-4,\n",
    "        fit_intercept=True\n",
    "    ),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"КРОСС-ВАЛИДАЦИЯ (5 фолдов)\")\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fold_jaccard_scores = []\n",
    "fold_f1_micro_scores = []\n",
    "fold_f1_macro_scores = []\n",
    "fold_hamming_losses = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train), 1):\n",
    "    print(f\"\\nФолд {fold}/5:\")\n",
    "    \n",
    "    X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "    \n",
    "    model_fold = MultiOutputClassifier(\n",
    "        LogisticRegression(\n",
    "            max_iter=1000,\n",
    "            class_weight='balanced',\n",
    "            C=0.8,\n",
    "            solver='liblinear',\n",
    "            random_state=42 + fold\n",
    "        ),\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    model_fold.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    y_pred_fold = model_fold.predict(X_val_fold)\n",
    "    \n",
    "    jaccard_fold = jaccard_score(y_val_fold, y_pred_fold, average='samples')\n",
    "    f1_micro_fold = f1_score(y_val_fold, y_pred_fold, average='micro')\n",
    "    f1_macro_fold = f1_score(y_val_fold, y_pred_fold, average='macro')\n",
    "    hamming_fold = hamming_loss(y_val_fold, y_pred_fold)\n",
    "    \n",
    "    fold_jaccard_scores.append(jaccard_fold)\n",
    "    fold_f1_micro_scores.append(f1_micro_fold)\n",
    "    fold_f1_macro_scores.append(f1_macro_fold)\n",
    "    fold_hamming_losses.append(hamming_fold)\n",
    "    \n",
    "    print(f\"  Jaccard: {jaccard_fold:.4f}, F1 Micro: {f1_micro_fold:.4f}, Hamming Loss: {hamming_fold:.4f}\")\n",
    "\n",
    "print(\"РЕЗУЛЬТАТЫ КРОСС-ВАЛИДАЦИИ\")\n",
    "\n",
    "print(f\"Jaccard Score (samples):\")\n",
    "print(f\"  Фолды: {[f'{v:.4f}' for v in fold_jaccard_scores]}\")\n",
    "print(f\"  Среднее: {np.mean(fold_jaccard_scores):.4f} (±{np.std(fold_jaccard_scores):.4f})\")\n",
    "\n",
    "print(f\"\\nF1 Micro:\")\n",
    "print(f\"  Фолды: {[f'{v:.4f}' for v in fold_f1_micro_scores]}\")\n",
    "print(f\"  Среднее: {np.mean(fold_f1_micro_scores):.4f} (±{np.std(fold_f1_micro_scores):.4f})\")\n",
    "\n",
    "print(f\"\\nF1 Macro:\")\n",
    "print(f\"  Фолды: {[f'{v:.4f}' for v in fold_f1_macro_scores]}\")\n",
    "print(f\"  Среднее: {np.mean(fold_f1_macro_scores):.4f} (±{np.std(fold_f1_macro_scores):.4f})\")\n",
    "\n",
    "print(f\"\\nHamming Loss:\")\n",
    "print(f\"  Фолды: {[f'{v:.4f}' for v in fold_hamming_losses]}\")\n",
    "print(f\"  Среднее: {np.mean(fold_hamming_losses):.4f} (±{np.std(fold_hamming_losses):.4f})\")\n",
    "\n",
    "print(\"ОБУЧЕНИЕ ФИНАЛЬНОЙ МОДЕЛИ\")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"ОЦЕНКА НА ТЕСТОВЫХ ДАННЫХ\")\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "\n",
    "jaccard = jaccard_score(y_test, y_pred, average='samples')\n",
    "hamming = hamming_loss(y_test, y_pred)\n",
    "f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "precision_micro = precision_score(y_test, y_pred, average='micro')\n",
    "recall_micro = recall_score(y_test, y_pred, average='micro')\n",
    "\n",
    "print(f\"Результаты на тестовых данных:\")\n",
    "print(f\"Jaccard Score (samples): {jaccard:.4f}\")\n",
    "print(f\"Hamming Loss: {hamming:.4f}\")\n",
    "print(f\"F1 Micro: {f1_micro:.4f}\")\n",
    "print(f\"F1 Macro: {f1_macro:.4f}\")\n",
    "print(f\"Precision Micro: {precision_micro:.4f}\")\n",
    "print(f\"Recall Micro: {recall_micro:.4f}\")\n",
    "print(f\"Классов: {y.shape[1]}, статей: {len(df)}\")\n",
    "\n",
    "print(\"ЭКСПЕРИМЕНТ С РАЗНЫМИ ПОРОГАМИ\")\n",
    "\n",
    "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "for threshold in thresholds:\n",
    "    y_pred_thresholded = np.array([(proba[:, 1] > threshold).astype(int) for proba in y_pred_proba]).T\n",
    "    jaccard_th = jaccard_score(y_test, y_pred_thresholded, average='samples')\n",
    "    print(f\"Порог {threshold}: Jaccard = {jaccard_th:.4f}\")\n",
    "\n",
    "print(\"СОХРАНЕНИЕ МОДЕЛИ\")\n",
    "\n",
    "joblib.dump(vectorizer, 'actual_helpers/vectorizer_last.pkl')\n",
    "joblib.dump(username_vectorizer, 'actual_helpers/username_vectorizer_last.pkl')\n",
    "joblib.dump(time_encoder, 'actual_helpers/time_encoder_last.pkl')\n",
    "joblib.dump(mlb_filtered, 'actual_helpers/mlb_filtered_last.pkl')\n",
    "joblib.dump(model, 'model_last.pkl')\n",
    "joblib.dump(scaler, 'actual_helpers/scaler_last.pkl')\n",
    "\n",
    "print(f\"Результаты кросс-валидации: Jaccard = {np.mean(fold_jaccard_scores):.4f} (±{np.std(fold_jaccard_scores):.4f})\")\n",
    "print(f\"Результаты на тесте: Jaccard = {jaccard:.4f}\")"
   ],
   "id": "fd4d7646c06affc7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статей после фильтрации: 98061, хабов: 393\n",
      "Распределение хабов: {np.int64(3): 39006, np.int64(4): 29976, np.int64(5): 23474, np.int64(2): 5465, np.int64(1): 133, np.int64(6): 7}\n",
      "\n",
      "Векторизация...\n",
      "Векторизация за 33.22 сек, X: (98035, 31555), y: (98035, 393)\n",
      "\n",
      "Размер train: (78428, 31555), test: (19607, 31555)\n",
      "\n",
      "Обучение модели...\n",
      "КРОСС-ВАЛИДАЦИЯ (5 фолдов)\n",
      "\n",
      "Фолд 1/5:\n",
      "  Jaccard: 0.3858, F1 Micro: 0.5206, Hamming Loss: 0.0125\n",
      "\n",
      "Фолд 2/5:\n",
      "  Jaccard: 0.3843, F1 Micro: 0.5196, Hamming Loss: 0.0125\n",
      "\n",
      "Фолд 3/5:\n",
      "  Jaccard: 0.3816, F1 Micro: 0.5154, Hamming Loss: 0.0126\n",
      "\n",
      "Фолд 4/5:\n",
      "  Jaccard: 0.3841, F1 Micro: 0.5191, Hamming Loss: 0.0126\n",
      "\n",
      "Фолд 5/5:\n",
      "  Jaccard: 0.3818, F1 Micro: 0.5170, Hamming Loss: 0.0126\n",
      "РЕЗУЛЬТАТЫ КРОСС-ВАЛИДАЦИИ\n",
      "Jaccard Score (samples):\n",
      "  Фолды: ['0.3858', '0.3843', '0.3816', '0.3841', '0.3818']\n",
      "  Среднее: 0.3835 (±0.0016)\n",
      "\n",
      "F1 Micro:\n",
      "  Фолды: ['0.5206', '0.5196', '0.5154', '0.5191', '0.5170']\n",
      "  Среднее: 0.5183 (±0.0019)\n",
      "\n",
      "F1 Macro:\n",
      "  Фолды: ['0.5385', '0.5386', '0.5314', '0.5382', '0.5363']\n",
      "  Среднее: 0.5366 (±0.0028)\n",
      "\n",
      "Hamming Loss:\n",
      "  Фолды: ['0.0125', '0.0125', '0.0126', '0.0126', '0.0126']\n",
      "  Среднее: 0.0126 (±0.0001)\n",
      "ОБУЧЕНИЕ ФИНАЛЬНОЙ МОДЕЛИ\n",
      "ОЦЕНКА НА ТЕСТОВЫХ ДАННЫХ\n",
      "\n",
      "Результаты на тестовых данных:\n",
      "Jaccard Score (samples): 0.3852\n",
      "Hamming Loss: 0.0128\n",
      "F1 Micro: 0.5188\n",
      "F1 Macro: 0.5400\n",
      "Precision Micro: 0.4031\n",
      "Recall Micro: 0.7274\n",
      "Классов: 393, статей: 98035\n",
      "ЭКСПЕРИМЕНТ С РАЗНЫМИ ПОРОГАМИ\n",
      "Порог 0.1: Jaccard = 0.1103\n",
      "Порог 0.2: Jaccard = 0.1965\n",
      "Порог 0.3: Jaccard = 0.2724\n",
      "Порог 0.4: Jaccard = 0.3350\n",
      "Порог 0.5: Jaccard = 0.3852\n",
      "СОХРАНЕНИЕ МОДЕЛИ\n",
      "Результаты кросс-валидации: Jaccard = 0.3835 (±0.0016)\n",
      "Результаты на тесте: Jaccard = 0.3852\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
